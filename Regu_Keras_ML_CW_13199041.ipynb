{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regu_Keras_ML_CW_13199041",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIdYxi-Re7ZT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from pandas import read_csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.layers import Dense, Dropout,Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.core import Activation\n",
        "from keras.optimizers import Optimizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.utils import to_categorical \n",
        "from keras import backend as K\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from numpy import reshape\n",
        "from tensorflow.keras import backend\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.regularizers import l1 , l2, l1_l2 \n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import kullback_leibler_divergence\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import tensorflow as tf \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from pandas import set_option\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from numpy import set_printoptions\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.regularizers import l1 \n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import kullback_leibler_divergence\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import RFE , chi2\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from scipy.stats import uniform\n",
        "from matplotlib import pyplot\n",
        "import tensorflow as tf \n",
        "import random as python_random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW8OIK4ufjiD"
      },
      "source": [
        "# Read the data set \n",
        "filename1 = 'adult.data'\n",
        "filename2 = 'adult.test'\n",
        "names = ('age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class')\n",
        "sample_train = pd.read_csv(filename1,names= names, na_values=[' ?','?'])\n",
        "sample_test = pd.read_csv(filename2, names= names, na_values=[' ?','?'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2Lq0g7xdQs",
        "outputId": "54e81cef-cbd5-4bd5-866c-b5f05519e478"
      },
      "source": [
        "# To see the shape of the  train dataset\n",
        "sample_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5G45jf-0S5q",
        "outputId": "236449c8-4b14-4822-dc44-45b02d08a7f7"
      },
      "source": [
        "# To see the shape of the  test dataset\n",
        "sample_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16282, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiF0KreFxhwu",
        "outputId": "bd5eb44d-319c-4e23-8b49-f1fe71815bde"
      },
      "source": [
        "# Here we drop the rows which has ' ?' in the test dataset\n",
        "data_train = sample_train.dropna()\n",
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCBkDiDH0JDV",
        "outputId": "e4824a98-c4f6-406f-daf7-0f9542b9fb12"
      },
      "source": [
        "# Here we drop the rows which has ' ?' in the train dataset\n",
        "data_test = sample_test.dropna()\n",
        "data_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "pZjG-6bD5NSv",
        "outputId": "7b3a657b-f258-4b38-91d4-668288f875d6"
      },
      "source": [
        "data_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  fnlwgt  ... hours-per-week  native-country   class\n",
              "0       39          State-gov   77516  ...             40   United-States   <=50K\n",
              "1       50   Self-emp-not-inc   83311  ...             13   United-States   <=50K\n",
              "2       38            Private  215646  ...             40   United-States   <=50K\n",
              "3       53            Private  234721  ...             40   United-States   <=50K\n",
              "4       28            Private  338409  ...             40            Cuba   <=50K\n",
              "...    ...                ...     ...  ...            ...             ...     ...\n",
              "32556   27            Private  257302  ...             38   United-States   <=50K\n",
              "32557   40            Private  154374  ...             40   United-States    >50K\n",
              "32558   58            Private  151910  ...             40   United-States   <=50K\n",
              "32559   22            Private  201490  ...             20   United-States   <=50K\n",
              "32560   52       Self-emp-inc  287927  ...             40   United-States    >50K\n",
              "\n",
              "[30162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "ZMWwRfhJSpzl",
        "outputId": "410a7e13-533b-4ad8-95eb-9508dbe54668"
      },
      "source": [
        "data_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802.0</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951.0</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>34</td>\n",
              "      <td>Private</td>\n",
              "      <td>198693.0</td>\n",
              "      <td>10th</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>33</td>\n",
              "      <td>Private</td>\n",
              "      <td>245211.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age      workclass    fnlwgt  ... hours-per-week  native-country    class\n",
              "1      25        Private  226802.0  ...           40.0   United-States   <=50K.\n",
              "2      38        Private   89814.0  ...           50.0   United-States   <=50K.\n",
              "3      28      Local-gov  336951.0  ...           40.0   United-States    >50K.\n",
              "4      44        Private  160323.0  ...           40.0   United-States    >50K.\n",
              "6      34        Private  198693.0  ...           30.0   United-States   <=50K.\n",
              "...    ..            ...       ...  ...            ...             ...      ...\n",
              "16276  33        Private  245211.0  ...           40.0   United-States   <=50K.\n",
              "16277  39        Private  215419.0  ...           36.0   United-States   <=50K.\n",
              "16279  38        Private  374983.0  ...           50.0   United-States   <=50K.\n",
              "16280  44        Private   83891.0  ...           40.0   United-States   <=50K.\n",
              "16281  35   Self-emp-inc  182148.0  ...           60.0   United-States    >50K.\n",
              "\n",
              "[15060 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSJ3iQEZ0oNi",
        "outputId": "043c8c99-dde3-483c-dacc-e018b7cf9ee3"
      },
      "source": [
        "# Here we check if there are any '?' values in any of the features\n",
        "for c in names:\n",
        "  print(c)\n",
        "  print(data_train[c].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "36    852\n",
            "31    851\n",
            "33    837\n",
            "34    836\n",
            "35    828\n",
            "     ... \n",
            "82      7\n",
            "83      5\n",
            "88      3\n",
            "85      3\n",
            "86      1\n",
            "Name: age, Length: 72, dtype: int64\n",
            "workclass\n",
            " Private             22286\n",
            " Self-emp-not-inc     2499\n",
            " Local-gov            2067\n",
            " State-gov            1279\n",
            " Self-emp-inc         1074\n",
            " Federal-gov           943\n",
            " Without-pay            14\n",
            "Name: workclass, dtype: int64\n",
            "fnlwgt\n",
            "203488    13\n",
            "113364    12\n",
            "164190    12\n",
            "123011    12\n",
            "148995    12\n",
            "          ..\n",
            "34393      1\n",
            "288341     1\n",
            "239415     1\n",
            "118352     1\n",
            "229376     1\n",
            "Name: fnlwgt, Length: 20263, dtype: int64\n",
            "education\n",
            " HS-grad         9840\n",
            " Some-college    6678\n",
            " Bachelors       5044\n",
            " Masters         1627\n",
            " Assoc-voc       1307\n",
            " 11th            1048\n",
            " Assoc-acdm      1008\n",
            " 10th             820\n",
            " 7th-8th          557\n",
            " Prof-school      542\n",
            " 9th              455\n",
            " 12th             377\n",
            " Doctorate        375\n",
            " 5th-6th          288\n",
            " 1st-4th          151\n",
            " Preschool         45\n",
            "Name: education, dtype: int64\n",
            "education-num\n",
            "9     9840\n",
            "10    6678\n",
            "13    5044\n",
            "14    1627\n",
            "11    1307\n",
            "7     1048\n",
            "12    1008\n",
            "6      820\n",
            "4      557\n",
            "15     542\n",
            "5      455\n",
            "8      377\n",
            "16     375\n",
            "3      288\n",
            "2      151\n",
            "1       45\n",
            "Name: education-num, dtype: int64\n",
            "marital-status\n",
            " Married-civ-spouse       14065\n",
            " Never-married             9726\n",
            " Divorced                  4214\n",
            " Separated                  939\n",
            " Widowed                    827\n",
            " Married-spouse-absent      370\n",
            " Married-AF-spouse           21\n",
            "Name: marital-status, dtype: int64\n",
            "occupation\n",
            " Prof-specialty       4038\n",
            " Craft-repair         4030\n",
            " Exec-managerial      3992\n",
            " Adm-clerical         3721\n",
            " Sales                3584\n",
            " Other-service        3212\n",
            " Machine-op-inspct    1966\n",
            " Transport-moving     1572\n",
            " Handlers-cleaners    1350\n",
            " Farming-fishing       989\n",
            " Tech-support          912\n",
            " Protective-serv       644\n",
            " Priv-house-serv       143\n",
            " Armed-Forces            9\n",
            "Name: occupation, dtype: int64\n",
            "relationship\n",
            " Husband           12463\n",
            " Not-in-family      7726\n",
            " Own-child          4466\n",
            " Unmarried          3212\n",
            " Wife               1406\n",
            " Other-relative      889\n",
            "Name: relationship, dtype: int64\n",
            "race\n",
            " White                 25933\n",
            " Black                  2817\n",
            " Asian-Pac-Islander      895\n",
            " Amer-Indian-Eskimo      286\n",
            " Other                   231\n",
            "Name: race, dtype: int64\n",
            "sex\n",
            " Male      20380\n",
            " Female     9782\n",
            "Name: sex, dtype: int64\n",
            "capital-gain\n",
            "0        27624\n",
            "15024      337\n",
            "7688       270\n",
            "7298       240\n",
            "99999      148\n",
            "         ...  \n",
            "401          1\n",
            "22040        1\n",
            "4931         1\n",
            "1455         1\n",
            "1639         1\n",
            "Name: capital-gain, Length: 118, dtype: int64\n",
            "capital-loss\n",
            "0       28735\n",
            "1902      194\n",
            "1977      162\n",
            "1887      155\n",
            "1848       50\n",
            "        ...  \n",
            "419         1\n",
            "1411        1\n",
            "1539        1\n",
            "2472        1\n",
            "2467        1\n",
            "Name: capital-loss, Length: 90, dtype: int64\n",
            "hours-per-week\n",
            "40    14251\n",
            "50     2718\n",
            "45     1753\n",
            "60     1405\n",
            "35     1184\n",
            "      ...  \n",
            "94        1\n",
            "87        1\n",
            "74        1\n",
            "82        1\n",
            "92        1\n",
            "Name: hours-per-week, Length: 94, dtype: int64\n",
            "native-country\n",
            " United-States                 27504\n",
            " Mexico                          610\n",
            " Philippines                     188\n",
            " Germany                         128\n",
            " Puerto-Rico                     109\n",
            " Canada                          107\n",
            " El-Salvador                     100\n",
            " India                           100\n",
            " Cuba                             92\n",
            " England                          86\n",
            " Jamaica                          80\n",
            " South                            71\n",
            " China                            68\n",
            " Italy                            68\n",
            " Dominican-Republic               67\n",
            " Vietnam                          64\n",
            " Guatemala                        63\n",
            " Japan                            59\n",
            " Columbia                         56\n",
            " Poland                           56\n",
            " Iran                             42\n",
            " Taiwan                           42\n",
            " Haiti                            42\n",
            " Portugal                         34\n",
            " Nicaragua                        33\n",
            " Peru                             30\n",
            " Greece                           29\n",
            " France                           27\n",
            " Ecuador                          27\n",
            " Ireland                          24\n",
            " Hong                             19\n",
            " Cambodia                         18\n",
            " Trinadad&Tobago                  18\n",
            " Thailand                         17\n",
            " Laos                             17\n",
            " Yugoslavia                       16\n",
            " Outlying-US(Guam-USVI-etc)       14\n",
            " Hungary                          13\n",
            " Honduras                         12\n",
            " Scotland                         11\n",
            " Holand-Netherlands                1\n",
            "Name: native-country, dtype: int64\n",
            "class\n",
            " <=50K    22654\n",
            " >50K      7508\n",
            "Name: class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfouFHiSS7Du",
        "outputId": "eb4a2262-e57c-4211-a304-ecb1b8856bae"
      },
      "source": [
        "for c in names:\n",
        "  print(c)\n",
        "  print(data_test[c].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "35    444\n",
            "33    442\n",
            "36    431\n",
            "31    423\n",
            "38    420\n",
            "     ... \n",
            "85      2\n",
            "88      2\n",
            "89      1\n",
            "84      1\n",
            "87      1\n",
            "Name: age, Length: 73, dtype: int64\n",
            "workclass\n",
            " Private             11021\n",
            " Self-emp-not-inc     1297\n",
            " Local-gov            1033\n",
            " State-gov             667\n",
            " Self-emp-inc          572\n",
            " Federal-gov           463\n",
            " Without-pay             7\n",
            "Name: workclass, dtype: int64\n",
            "fnlwgt\n",
            "136986.0    9\n",
            "203488.0    8\n",
            "127651.0    8\n",
            "120277.0    8\n",
            "125892.0    8\n",
            "           ..\n",
            "153813.0    1\n",
            "242589.0    1\n",
            "227540.0    1\n",
            "77143.0     1\n",
            "204244.0    1\n",
            "Name: fnlwgt, Length: 11913, dtype: int64\n",
            "education\n",
            " HS-grad         4943\n",
            " Some-college    3221\n",
            " Bachelors       2526\n",
            " Masters          887\n",
            " Assoc-voc        652\n",
            " 11th             571\n",
            " Assoc-acdm       499\n",
            " 10th             403\n",
            " 7th-8th          266\n",
            " Prof-school      243\n",
            " 9th              221\n",
            " 12th             200\n",
            " Doctorate        169\n",
            " 5th-6th          161\n",
            " 1st-4th           71\n",
            " Preschool         27\n",
            "Name: education, dtype: int64\n",
            "education-num\n",
            "9.0     4943\n",
            "10.0    3221\n",
            "13.0    2526\n",
            "14.0     887\n",
            "11.0     652\n",
            "7.0      571\n",
            "12.0     499\n",
            "6.0      403\n",
            "4.0      266\n",
            "15.0     243\n",
            "5.0      221\n",
            "8.0      200\n",
            "16.0     169\n",
            "3.0      161\n",
            "2.0       71\n",
            "1.0       27\n",
            "Name: education-num, dtype: int64\n",
            "marital-status\n",
            " Married-civ-spouse       6990\n",
            " Never-married            4872\n",
            " Divorced                 2083\n",
            " Separated                 472\n",
            " Widowed                   450\n",
            " Married-spouse-absent     182\n",
            " Married-AF-spouse          11\n",
            "Name: marital-status, dtype: int64\n",
            "occupation\n",
            " Exec-managerial      1992\n",
            " Craft-repair         1990\n",
            " Prof-specialty       1970\n",
            " Sales                1824\n",
            " Adm-clerical         1819\n",
            " Other-service        1596\n",
            " Machine-op-inspct    1004\n",
            " Transport-moving      744\n",
            " Handlers-cleaners     696\n",
            " Tech-support          508\n",
            " Farming-fishing       491\n",
            " Protective-serv       332\n",
            " Priv-house-serv        89\n",
            " Armed-Forces            5\n",
            "Name: occupation, dtype: int64\n",
            "relationship\n",
            " Husband           6203\n",
            " Not-in-family     3976\n",
            " Own-child         2160\n",
            " Unmarried         1576\n",
            " Wife               685\n",
            " Other-relative     460\n",
            "Name: relationship, dtype: int64\n",
            "race\n",
            " White                 12970\n",
            " Black                  1411\n",
            " Asian-Pac-Islander      408\n",
            " Amer-Indian-Eskimo      149\n",
            " Other                   122\n",
            "Name: race, dtype: int64\n",
            "sex\n",
            " Male      10147\n",
            " Female     4913\n",
            "Name: sex, dtype: int64\n",
            "capital-gain\n",
            "0.0        13808\n",
            "15024.0      161\n",
            "7688.0       121\n",
            "7298.0       111\n",
            "99999.0       81\n",
            "           ...  \n",
            "41310.0        1\n",
            "991.0          1\n",
            "1173.0         1\n",
            "34095.0        1\n",
            "2062.0         1\n",
            "Name: capital-gain, Length: 110, dtype: int64\n",
            "capital-loss\n",
            "0.0       14347\n",
            "1902.0      100\n",
            "1977.0       84\n",
            "1887.0       73\n",
            "2415.0       23\n",
            "          ...  \n",
            "653.0         1\n",
            "1911.0        1\n",
            "2547.0        1\n",
            "2282.0        1\n",
            "2603.0        1\n",
            "Name: capital-loss, Length: 79, dtype: int64\n",
            "hours-per-week\n",
            "40.0    7107\n",
            "50.0    1376\n",
            "45.0     849\n",
            "60.0     680\n",
            "35.0     592\n",
            "        ... \n",
            "89.0       1\n",
            "69.0       1\n",
            "79.0       1\n",
            "76.0       1\n",
            "73.0       1\n",
            "Name: hours-per-week, Length: 89, dtype: int64\n",
            "native-country\n",
            " United-States                 13788\n",
            " Mexico                          293\n",
            " Philippines                      95\n",
            " Puerto-Rico                      66\n",
            " Germany                          65\n",
            " Canada                           56\n",
            " India                            47\n",
            " El-Salvador                      47\n",
            " China                            45\n",
            " Cuba                             41\n",
            " England                          33\n",
            " Italy                            32\n",
            " South                            30\n",
            " Dominican-Republic               30\n",
            " Japan                            30\n",
            " Portugal                         28\n",
            " Haiti                            27\n",
            " Columbia                         26\n",
            " Poland                           25\n",
            " Guatemala                        23\n",
            " Jamaica                          23\n",
            " Greece                           20\n",
            " Vietnam                          19\n",
            " Ecuador                          16\n",
            " Peru                             15\n",
            " Nicaragua                        15\n",
            " Iran                             14\n",
            " Taiwan                           13\n",
            " Thailand                         12\n",
            " Ireland                          12\n",
            " Scotland                          9\n",
            " Hong                              9\n",
            " France                            9\n",
            " Trinadad&Tobago                   8\n",
            " Outlying-US(Guam-USVI-etc)        8\n",
            " Cambodia                          8\n",
            " Yugoslavia                        7\n",
            " Honduras                          7\n",
            " Hungary                           5\n",
            " Laos                              4\n",
            "Name: native-country, dtype: int64\n",
            "class\n",
            " <=50K.    11360\n",
            " >50K.      3700\n",
            "Name: class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMlY8DGWAAxl"
      },
      "source": [
        "# Using Lable Encoder on train data to convert the Categorical data into Numerical data\n",
        "data_train_new = data_train.apply(LabelEncoder().fit_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "KxGh35KKAKPe",
        "outputId": "8dee6ff8-3df5-4fd6-c0ee-19418f71eb04"
      },
      "source": [
        "data_train_new\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "      <td>2491</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>2727</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>13188</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>14354</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>18120</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>15471</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>7555</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>7377</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>12060</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>16689</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  fnlwgt  ...  hours-per-week  native-country  class\n",
              "0       22          5    2491  ...              39              38      0\n",
              "1       33          4    2727  ...              12              38      0\n",
              "2       21          2   13188  ...              39              38      0\n",
              "3       36          2   14354  ...              39              38      0\n",
              "4       11          2   18120  ...              39               4      0\n",
              "...    ...        ...     ...  ...             ...             ...    ...\n",
              "32556   10          2   15471  ...              37              38      0\n",
              "32557   23          2    7555  ...              39              38      1\n",
              "32558   41          2    7377  ...              39              38      0\n",
              "32559    5          2   12060  ...              19              38      0\n",
              "32560   35          3   16689  ...              39              38      1\n",
              "\n",
              "[30162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY84JiCFTr7F"
      },
      "source": [
        "# Using Lable Encoder on train data to convert the Categorical data into Numerical data\n",
        "data_test_new = data_test.apply(LabelEncoder().fit_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "dYCHC5mHTyAN",
        "outputId": "1bcb71d7-44d4-4468-d16b-06616d388fdf"
      },
      "source": [
        "data_test_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8315</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>1754</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>10750</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>4780</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>7091</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>8927</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>7893</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>11193</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>1593</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>6062</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  fnlwgt  ...  hours-per-week  native-country  class\n",
              "1        8          2    8315  ...              39              37      0\n",
              "2       21          2    1754  ...              49              37      0\n",
              "3       11          1   10750  ...              39              37      1\n",
              "4       27          2    4780  ...              39              37      1\n",
              "6       17          2    7091  ...              29              37      0\n",
              "...    ...        ...     ...  ...             ...             ...    ...\n",
              "16276   16          2    8927  ...              39              37      0\n",
              "16277   22          2    7893  ...              35              37      0\n",
              "16279   21          2   11193  ...              49              37      0\n",
              "16280   27          2    1593  ...              39              37      0\n",
              "16281   18          3    6062  ...              59              37      1\n",
              "\n",
              "[15060 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCpMyMWRT-r7"
      },
      "source": [
        "Splitting train data into array and using MinMaxScaler to scale the data\n",
        " & \n",
        "Splitting test data into array and using MinMaxScaler to scale the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugv3BRf4CN5c"
      },
      "source": [
        "# Training data \n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "array = data_train_new.values\n",
        "# Seperate array into input and output\n",
        "X = array[:, 0:14]\n",
        "Y = array[:, 14]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rX = scaler.fit_transform(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXfV2qFtUY-q"
      },
      "source": [
        "# FOR Test data\n",
        "array = data_test_new.values\n",
        "# Seperate array into input and output\n",
        "X_t = array[:, 0:14]\n",
        "Y_t = array[:, 14]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rX_t = scaler.fit_transform(X_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3d04GDPWcbv"
      },
      "source": [
        "Filter Method - SelectKBest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNMMOjJLIHR5"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "# feature extraction\n",
        "#Select k best on train set \n",
        "selectModel = SelectKBest(score_func=chi2, k=9)\n",
        "kBesttrain = selectModel.fit(rX,Y)\n",
        "selectTrainFeatures = kBesttrain.transform(rX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NKHtafhV7Lx",
        "outputId": "f683d386-ec7b-4586-9d4d-61471f8ecabd"
      },
      "source": [
        "selectTrainFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwk81Ua6VlLJ"
      },
      "source": [
        "#Select k best on test set \n",
        "kBesttest = selectModel.fit(rX_t,Y_t)\n",
        "selectTestFeatures = kBesttest.transform(rX_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2GU_XTgWAaD",
        "outputId": "da416b06-23e6-4e98-f755-e0225d203292"
      },
      "source": [
        "selectTestFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6hfGgKqV3Qo"
      },
      "source": [
        "Wrapper Method - Recurssive Feature Elemination (RFE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yoDwiszXBmJ"
      },
      "source": [
        "#RFE on train set\n",
        "model_rfe = LogisticRegression(solver='liblinear') \n",
        "rfeModel = RFE(model_rfe, 9) \n",
        "rfeTrain = rfeModel.fit(rX,Y)\n",
        "rfeTrainFeatures = rfeTrain.transform(rX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS9r8X4RXQ_m",
        "outputId": "6c68c9b4-0588-4480-8f7e-65b303a4ca27"
      },
      "source": [
        "rfeTrainFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6I0_fbPXZGT"
      },
      "source": [
        "#RFE on test set\n",
        "rfeTest = rfeModel.fit(rX_t,Y_t)\n",
        "rfeTestFeatures = rfeTest.transform(rX_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNf3NitLXlUX",
        "outputId": "bac64c99-5872-47e8-a544-05841cdcba6a"
      },
      "source": [
        "rfeTestFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfcTjYEXYpkL"
      },
      "source": [
        "Embedded method - using ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiG2SdaVYuHn"
      },
      "source": [
        "#ExtraTree on train set\n",
        "extraTreeModel = ExtraTreesClassifier(n_estimators=9)\n",
        "exTreeTrain = extraTreeModel.fit(rX, Y)\n",
        "clf_model = SelectFromModel(exTreeTrain, prefit=True)\n",
        "exTreeTrainFeatures = clf_model.transform(rX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO1R9zvBY_oL",
        "outputId": "046c6d80-0610-4f59-8be6-7ae2e303bca4"
      },
      "source": [
        "exTreeTrainFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcQSTfe9Zgv2"
      },
      "source": [
        "#ExtraTree on test set\n",
        "exTreeTest = extraTreeModel.fit(rX_t, Y_t)\n",
        "clf_modeltest = SelectFromModel(exTreeTest, prefit=True)\n",
        "exTreeTestFeatures = clf_modeltest.transform(rX_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcVknE8lZnM8",
        "outputId": "304f7ced-a69c-40bf-e47c-27a19632f8c4"
      },
      "source": [
        "exTreeTestFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mLow_knXrZ6"
      },
      "source": [
        "Principal component Analysis (PCA) from the output of Filter (SelectKBest), Wrapper (RFE) & Embedded (ExtraTreesClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYdy-1RfcfS_"
      },
      "source": [
        "PCA -Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IBh5Rk9Zy19"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Filter (SelectKBest)\n",
        "pca = PCA(n_components=5, random_state = 7) \n",
        "fit_pca_train = pca.fit(selectTrainFeatures) \n",
        "pca_filter_train_features = fit_pca_train.transform(selectTrainFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKAbaXFra-uF"
      },
      "source": [
        "## On selected test feature dataset for PCA # \n",
        "fit_pca_test = pca.fit(selectTestFeatures) \n",
        "pca_filter_test_features = fit_pca_test.transform(selectTestFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR3o0OkcbBCc",
        "outputId": "02ae15c5-6d19-4378-d00a-b307912f2213"
      },
      "source": [
        "pca_filter_train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcc8tv3tbIeB",
        "outputId": "c12fc31b-e89e-4894-c070-188a66dbfe34"
      },
      "source": [
        "pca_filter_test_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8QR-XHOci66"
      },
      "source": [
        "PCA -Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7WyY-9bRSx"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Wrapper (RFE) \n",
        "fit_pca_train = pca.fit(rfeTrainFeatures) \n",
        "pca_wrapper_train_features = fit_pca_train.transform(rfeTrainFeatures)\n",
        "fit_pca_test = pca.fit(rfeTestFeatures) \n",
        "pca_wrapper_test_features = fit_pca_test.transform(rfeTestFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpH0xsO5cVBh",
        "outputId": "05377c2c-a871-4384-d782-c03cc2ca302f"
      },
      "source": [
        "pca_wrapper_train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GdP3yLHcX_P",
        "outputId": "e5142255-d423-4216-abe0-3bb5b9ca4a0b"
      },
      "source": [
        "pca_wrapper_test_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eaLw7jrcnre"
      },
      "source": [
        "PCA - Embedded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2ca12Wecs-u"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Wrapper (RFE) \n",
        "pca = PCA(n_components=5, random_state = 2) \n",
        "fit_pca_train = pca.fit(exTreeTrainFeatures) \n",
        "pca_embedded_train_features = fit_pca_train.transform(exTreeTrainFeatures)\n",
        "fit_pca_test = pca.fit(exTreeTestFeatures) \n",
        "pca_embedded_test_features = fit_pca_test.transform(exTreeTestFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz1bwS2nft1O",
        "outputId": "c1a3a238-69d5-48e2-abac-668ffd46910f"
      },
      "source": [
        "pca_embedded_train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn7iv6c3fzyC",
        "outputId": "b0a2881b-9a51-4ab9-ce13-c26d8d12f173"
      },
      "source": [
        "pca_embedded_test_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0u5B0Xf4cl"
      },
      "source": [
        "###Using Neural Network Model on the PCA output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RduO9SkThMjG"
      },
      "source": [
        "Training with PCA - FILTER METHOD output \"pca_filter_train_features \" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaLIsicrgCrN",
        "outputId": "5062e512-039e-4ecf-9151-b3d135525a7a"
      },
      "source": [
        "optimizers = ['adam', 'rmsprop']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model1(optimizer=optimizers, init=inits):\n",
        "  # creatoe model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model1, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_filter_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.808335 using {'batch_size': 30, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rc-EAeRhqws"
      },
      "source": [
        "Training with PCA - WRPPER METHOD output \"pca_wrapper_filter_features\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muyaJG7MiBvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a07a56b-1e91-4f82-9f5d-2661e4c9f1bc"
      },
      "source": [
        "optimizers = ['adam', 'rmsprop']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model3(optimizer=optimizers, init=inits):\n",
        "  # creatoe model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model3, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_wrapper_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.827134 using {'batch_size': 30, 'epochs': 10, 'init': 'uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK1C3kI8ZCZ7"
      },
      "source": [
        "Training with PCA - EMBEDDED METHOD output \"pca_embedded_train_features\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf6q3KamZRHr",
        "outputId": "7b07077e-e05c-4c7e-a889-180e98dd42e2"
      },
      "source": [
        "optimizers = ['adam', 'rmsprop']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model4(optimizer=optimizers, init=inits):\n",
        "  # creatoe model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model4, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.815894 using {'batch_size': 30, 'epochs': 10, 'init': 'uniform', 'optimizer': 'rmsprop'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFndcobgUkxa"
      },
      "source": [
        "###Creating WAME algorithm  with changing Learning Rate  so we create WAME1, WAME2, WAME3, WAME4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4aVWA6eUjGC"
      },
      "source": [
        "class WAME1(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME1, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): # everything betweem 4 and 12\n",
        "                                       # \"the for loop in the paper\"\n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       # 4\n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g #4 & 6\n",
        "           change_below_zero = K.less(change,0.) #true or false\n",
        "           change_above_zero = K.greater(change,0.) #true or false\n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, # if we jump to line 6\n",
        "               a * self.eta_minus, # use line 7\n",
        "               K.switch(change_above_zero,  # if line 4 if true\n",
        "                        a * self.eta_plus, # line 5 \n",
        "                        a) # otheriwse leave a as it was\n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) #end of line 5 and 7 (min and max)\n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) #9\n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped #10\n",
        "           a_rate = a_clipped / am_t #am_t == θij(t) # 11\n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) #12\n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME1, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woOJNepbeXOr"
      },
      "source": [
        "class WAME2(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.0001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME2, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): # everything betweem 4 and 12\n",
        "                                       # \"the for loop in the paper\"\n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       # 4\n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g #4 & 6\n",
        "           change_below_zero = K.less(change,0.) #true or false\n",
        "           change_above_zero = K.greater(change,0.) #true or false\n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, # if we jump to line 6\n",
        "               a * self.eta_minus, # use line 7\n",
        "               K.switch(change_above_zero,  # if line 4 if true\n",
        "                        a * self.eta_plus, # line 5 \n",
        "                        a) # otheriwse leave a as it was\n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) #end of line 5 and 7 (min and max)\n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) #9\n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped #10\n",
        "           a_rate = a_clipped / am_t #am_t == θij(t) # 11\n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) #12\n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME2, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h2YV1_JesrD"
      },
      "source": [
        "class WAME3(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.00001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME3, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): # everything betweem 4 and 12\n",
        "                                       # \"the for loop in the paper\"\n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       # 4\n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g #4 & 6\n",
        "           change_below_zero = K.less(change,0.) #true or false\n",
        "           change_above_zero = K.greater(change,0.) #true or false\n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, # if we jump to line 6\n",
        "               a * self.eta_minus, # use line 7\n",
        "               K.switch(change_above_zero,  # if line 4 if true\n",
        "                        a * self.eta_plus, # line 5 \n",
        "                        a) # otheriwse leave a as it was\n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) #end of line 5 and 7 (min and max)\n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) #9\n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped #10\n",
        "           a_rate = a_clipped / am_t #am_t == θij(t) # 11\n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) #12\n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME3, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axb4iOiefHHU"
      },
      "source": [
        "class WAME4(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.000001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME4, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): # everything betweem 4 and 12\n",
        "                                       # \"the for loop in the paper\"\n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       # 4\n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g #4 & 6\n",
        "           change_below_zero = K.less(change,0.) #true or false\n",
        "           change_above_zero = K.greater(change,0.) #true or false\n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, # if we jump to line 6\n",
        "               a * self.eta_minus, # use line 7\n",
        "               K.switch(change_above_zero,  # if line 4 if true\n",
        "                        a * self.eta_plus, # line 5 \n",
        "                        a) # otheriwse leave a as it was\n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) #end of line 5 and 7 (min and max)\n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) #9\n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped #10\n",
        "           a_rate = a_clipped / am_t #am_t == θij(t) # 11\n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) #12\n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME4, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eRe9CEEj1pA"
      },
      "source": [
        "Configering The Keras Model For Filter, Wrapper and Embedded PCA Output and Geetting the best tune results in from Grid search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFSIoEQmpuUs"
      },
      "source": [
        "Training with PCA - FILTER METHOD output \"pca_filter_train_features \" with WAME 1, 2 , 3 & 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp73889fpUVE",
        "outputId": "8e7de87c-733b-43af-ba4c-a7466c4752b7"
      },
      "source": [
        "optimizers = ['adam', 'rmsprop', 'WAME1', 'WAME2', 'WAME3', 'WAME4']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model5(optimizer=optimizers, init=inits):\n",
        "  # creatoe model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model5, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_filter_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.808700 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjDdADQJqIg_"
      },
      "source": [
        "Training with PCA - WRPPER METHOD output \"pca_wrapper_filter_features\" with WAME 1, 2, 3 & 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LZhbOKIqW_r",
        "outputId": "0b29674c-0987-42b5-dff1-2fe9b660e5af"
      },
      "source": [
        "optimizers = ['adam', 'rmsprop', 'WAME1', 'WAME2', 'WAME3', 'WAME4']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model6(optimizer=optimizers, init=inits):\n",
        "  # creatoe model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model6, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_wrapper_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.828095 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME4'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJVK9ziYq6R6"
      },
      "source": [
        "Training with PCA - EMBEDDED METHOD output \"pca_embedded_train_features\" with WAME 1, 2, 3 & 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPrUMdRpq6-H",
        "outputId": "fb718cf6-0e18-483f-c2f1-8285e7a9a5de"
      },
      "source": [
        "optimizers = ['adam', 'rmsprop', 'WAME1', 'WAME2', 'WAME3', 'WAME4']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model7(optimizer=optimizers, init=inits):\n",
        "  # creatoe model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model7, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.817651 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rjuVv1yr6q0"
      },
      "source": [
        "Comparing best models (Betweent Filter, Wrapper & Embedded with WAME Output)  :\n",
        "\n",
        "(a) Using Crossvalidation on hyperparameter tuned models of Keras Classifier (from output of Filerr, Wrapper & Embedded) and compare the performance accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bmDJCdSPsbR9",
        "outputId": "9bc70920-2bbb-4325-9aef-1f0c7bedcd97"
      },
      "source": [
        "# The Keras on Filter output  \n",
        "# Best: 0.808700 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME1'}\n",
        "models1 = []\n",
        "models1.append(('Keras Classifier - Filter ', KerasClassifier(build_fn=create_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME1')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model1 in models1:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model1, pca_filter_train_features, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2341 - accuracy: 0.7359\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1639 - accuracy: 0.7704\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1600 - accuracy: 0.7904\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1579 - accuracy: 0.7914\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1539 - accuracy: 0.7981\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1515 - accuracy: 0.8026\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.8029\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1473 - accuracy: 0.8052\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1471 - accuracy: 0.8038\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.8028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2327 - accuracy: 0.7270\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1642 - accuracy: 0.7791\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1569 - accuracy: 0.8018\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1552 - accuracy: 0.7987\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1537 - accuracy: 0.7982\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1480 - accuracy: 0.8067\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1481 - accuracy: 0.8023\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1475 - accuracy: 0.8055\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1435 - accuracy: 0.8080\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1448 - accuracy: 0.8044\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2416 - accuracy: 0.7241\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1616 - accuracy: 0.7935\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1581 - accuracy: 0.7977\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1511 - accuracy: 0.8053\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1510 - accuracy: 0.8037\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1468 - accuracy: 0.8087\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1466 - accuracy: 0.8076\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1451 - accuracy: 0.8083\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1439 - accuracy: 0.8070\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1433 - accuracy: 0.8076\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2384 - accuracy: 0.7326\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1621 - accuracy: 0.7596\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1572 - accuracy: 0.7924\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1534 - accuracy: 0.8037\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1516 - accuracy: 0.7991\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1522 - accuracy: 0.8019\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1483 - accuracy: 0.8068\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1469 - accuracy: 0.8069\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1440 - accuracy: 0.8079\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1447 - accuracy: 0.8089\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2252 - accuracy: 0.7464\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1626 - accuracy: 0.7913\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1565 - accuracy: 0.8001\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1513 - accuracy: 0.8036\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1515 - accuracy: 0.8005\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.8026\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1490 - accuracy: 0.8001\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.8089\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.8097\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.8112\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2403 - accuracy: 0.7431\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1657 - accuracy: 0.7930\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1579 - accuracy: 0.7999\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1564 - accuracy: 0.7987\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.8023\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1500 - accuracy: 0.8038\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1462 - accuracy: 0.8086\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1450 - accuracy: 0.8063\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1456 - accuracy: 0.8055\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1435 - accuracy: 0.8092\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2261 - accuracy: 0.7486\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1660 - accuracy: 0.7879\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1588 - accuracy: 0.7991\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.8009\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1515 - accuracy: 0.8036\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1514 - accuracy: 0.8021\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1488 - accuracy: 0.8051\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1483 - accuracy: 0.8009\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1475 - accuracy: 0.8022\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1440 - accuracy: 0.8071\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2380 - accuracy: 0.7275\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1658 - accuracy: 0.7888\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1592 - accuracy: 0.7942\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1551 - accuracy: 0.7975\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1539 - accuracy: 0.7964\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1516 - accuracy: 0.7974\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1482 - accuracy: 0.8025\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.7995\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1475 - accuracy: 0.8001\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.8001\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2252 - accuracy: 0.7493\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1638 - accuracy: 0.7884\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1585 - accuracy: 0.7977\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1558 - accuracy: 0.7972\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1499 - accuracy: 0.8052\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1500 - accuracy: 0.8003\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1483 - accuracy: 0.8044\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8019\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1445 - accuracy: 0.8086\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1440 - accuracy: 0.8082\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2367 - accuracy: 0.7500\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1651 - accuracy: 0.7894\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1601 - accuracy: 0.7932\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1543 - accuracy: 0.7995\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1528 - accuracy: 0.8000\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1518 - accuracy: 0.8016\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.8045\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1474 - accuracy: 0.8054\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1479 - accuracy: 0.8011\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1466 - accuracy: 0.8030\n",
            "Keras Classifier - Filter : 0.806280 (0.005541)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Eg8v7PN6t272",
        "outputId": "12e9d193-487c-4e4a-aa1b-7b5e85aa4361"
      },
      "source": [
        "# The Keras on Wrapper output\n",
        "#Best: 0.828095 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME4'}\n",
        "models2 = []\n",
        "models2.append(('Keras Classifier - Wrapper ', KerasClassifier(build_fn=create_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME4')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model2 in models2:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model2, pca_wrapper_train_features, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2457 - accuracy: 0.7293\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1615 - accuracy: 0.8044\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1558 - accuracy: 0.8157\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1532 - accuracy: 0.8104\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1497 - accuracy: 0.8194\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1456 - accuracy: 0.8266\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1403 - accuracy: 0.8315\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1437 - accuracy: 0.8238\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1408 - accuracy: 0.8292\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1397 - accuracy: 0.8261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2185 - accuracy: 0.7524\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1642 - accuracy: 0.7949\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1551 - accuracy: 0.8199\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.8259\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1479 - accuracy: 0.8244\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1441 - accuracy: 0.8258\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1440 - accuracy: 0.8256\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1393 - accuracy: 0.8316\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1400 - accuracy: 0.8255\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1391 - accuracy: 0.8274\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2251 - accuracy: 0.7515\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1610 - accuracy: 0.7976\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1547 - accuracy: 0.8128\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1507 - accuracy: 0.8214\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1487 - accuracy: 0.8182\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1444 - accuracy: 0.8267\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.8264\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1443 - accuracy: 0.8267\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1419 - accuracy: 0.8265\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1395 - accuracy: 0.8280\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2380 - accuracy: 0.7359\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1628 - accuracy: 0.8013\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1549 - accuracy: 0.8123\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1510 - accuracy: 0.8176\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1484 - accuracy: 0.8201\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1473 - accuracy: 0.8205\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1438 - accuracy: 0.8255\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.8232\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1423 - accuracy: 0.8237\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1407 - accuracy: 0.8265\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2272 - accuracy: 0.7661\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1606 - accuracy: 0.8081\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1536 - accuracy: 0.8210\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1506 - accuracy: 0.8216\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1492 - accuracy: 0.8205\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.8231\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.8231\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1419 - accuracy: 0.8283\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1416 - accuracy: 0.8265\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1411 - accuracy: 0.8261\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2526 - accuracy: 0.7177\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1603 - accuracy: 0.8046\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1542 - accuracy: 0.8157\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.8277\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1471 - accuracy: 0.8267\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1446 - accuracy: 0.8278\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.8262\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1409 - accuracy: 0.8273\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1403 - accuracy: 0.8277\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1389 - accuracy: 0.8303\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2311 - accuracy: 0.7714\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1605 - accuracy: 0.8098\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1538 - accuracy: 0.8159\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1511 - accuracy: 0.8208\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1490 - accuracy: 0.8215\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1459 - accuracy: 0.8248\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1451 - accuracy: 0.8250\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1413 - accuracy: 0.8303\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1425 - accuracy: 0.8249\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1407 - accuracy: 0.8258\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2415 - accuracy: 0.7125\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1661 - accuracy: 0.7986\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1551 - accuracy: 0.8158\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1504 - accuracy: 0.8215\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1478 - accuracy: 0.8235\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1441 - accuracy: 0.8268\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1426 - accuracy: 0.8296\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1418 - accuracy: 0.8251\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1404 - accuracy: 0.8236\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1389 - accuracy: 0.8280\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2289 - accuracy: 0.7450\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1637 - accuracy: 0.7865\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1560 - accuracy: 0.8106\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1500 - accuracy: 0.8189\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1465 - accuracy: 0.8271\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1451 - accuracy: 0.8299\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1434 - accuracy: 0.8277\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.8266\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1408 - accuracy: 0.8278\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1405 - accuracy: 0.8255\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2297 - accuracy: 0.7504\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1637 - accuracy: 0.8001\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1543 - accuracy: 0.8199\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1502 - accuracy: 0.8229\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1481 - accuracy: 0.8254\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1443 - accuracy: 0.8288\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.8265\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1418 - accuracy: 0.8290\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1400 - accuracy: 0.8283\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1404 - accuracy: 0.8258\n",
            "Keras Classifier - Wrapper : 0.826039 (0.005873)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-2kg0B_suBxg",
        "outputId": "7ef1f4d5-6fe9-45ad-abd1-00a807d63a87"
      },
      "source": [
        "# The Keras on Embedded Method\n",
        "#Best: 0.817651 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
        "models3 = []\n",
        "models3.append(('Keras Classifier - Embedded ', KerasClassifier(build_fn=create_model7, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='adam')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model3 in models3:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True)\n",
        "  cv_results = cross_val_score(model3, pca_embedded_train_features, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2341 - accuracy: 0.7454\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1693 - accuracy: 0.7747\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1604 - accuracy: 0.7947\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1565 - accuracy: 0.8017\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.8089\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1507 - accuracy: 0.8109\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1509 - accuracy: 0.8117\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1493 - accuracy: 0.8095\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8098\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1448 - accuracy: 0.8175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2327 - accuracy: 0.7513\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1700 - accuracy: 0.7696\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1606 - accuracy: 0.7926\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1570 - accuracy: 0.8006\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1545 - accuracy: 0.8037\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1491 - accuracy: 0.8126\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1514 - accuracy: 0.8089\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1503 - accuracy: 0.8043\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1482 - accuracy: 0.8126\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1472 - accuracy: 0.8126\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2403 - accuracy: 0.7324\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1666 - accuracy: 0.7913\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1598 - accuracy: 0.7995\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1553 - accuracy: 0.8045\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1502 - accuracy: 0.8111\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1499 - accuracy: 0.8121\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.8074\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1467 - accuracy: 0.8147\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1458 - accuracy: 0.8152\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1439 - accuracy: 0.8179\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2417 - accuracy: 0.7442\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1668 - accuracy: 0.7871\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1593 - accuracy: 0.7971\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1593 - accuracy: 0.7967\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1545 - accuracy: 0.8031\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1496 - accuracy: 0.8110\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.8111\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1491 - accuracy: 0.8099\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1469 - accuracy: 0.8134\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1470 - accuracy: 0.8131\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2414 - accuracy: 0.7388\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1685 - accuracy: 0.7860\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1630 - accuracy: 0.7935\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1544 - accuracy: 0.8048\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1534 - accuracy: 0.8072\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1507 - accuracy: 0.8109\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1496 - accuracy: 0.8095\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.8156\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1467 - accuracy: 0.8134\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1479 - accuracy: 0.8080\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2360 - accuracy: 0.7409\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1680 - accuracy: 0.7735\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1571 - accuracy: 0.8017\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1556 - accuracy: 0.8048\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1529 - accuracy: 0.8079\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1509 - accuracy: 0.8110\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1479 - accuracy: 0.8124\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8133\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1477 - accuracy: 0.8108\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1435 - accuracy: 0.8206\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2340 - accuracy: 0.7611\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1694 - accuracy: 0.7826\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1609 - accuracy: 0.7934\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1563 - accuracy: 0.8042\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1554 - accuracy: 0.8007\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1513 - accuracy: 0.8104\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1510 - accuracy: 0.8092\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1482 - accuracy: 0.8131\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8140\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1475 - accuracy: 0.8156\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2339 - accuracy: 0.7481\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1652 - accuracy: 0.7899\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1586 - accuracy: 0.8002\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1549 - accuracy: 0.8039\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1517 - accuracy: 0.8073\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1516 - accuracy: 0.8088\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1488 - accuracy: 0.8109\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1467 - accuracy: 0.8165\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1455 - accuracy: 0.8142\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1475 - accuracy: 0.8119\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2402 - accuracy: 0.7532\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1699 - accuracy: 0.7673\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1636 - accuracy: 0.7874\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1575 - accuracy: 0.7972\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1558 - accuracy: 0.8011\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1523 - accuracy: 0.8057\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1489 - accuracy: 0.8138\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1490 - accuracy: 0.8123\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1463 - accuracy: 0.8151\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8138\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.2303 - accuracy: 0.7598\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1671 - accuracy: 0.7893\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1610 - accuracy: 0.7962\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1574 - accuracy: 0.8004\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1538 - accuracy: 0.8022\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1537 - accuracy: 0.8013\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1478 - accuracy: 0.8120\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1485 - accuracy: 0.8104\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8090\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1485 - accuracy: 0.8089\n",
            "Keras Classifier - Embedded : 0.814933 (0.007052)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGQ7TJIevlbu"
      },
      "source": [
        "b) Predicting labels of test data instances and evaluating model based on :\n",
        "\n",
        "(i) Time taken to build model\n",
        "\n",
        "(ii) Time taken to test model\n",
        "\n",
        "(iii) Model Accuracy\n",
        "\n",
        "(iv) Model Error Rate\n",
        "\n",
        "(v) DetectionRate\n",
        "\n",
        "(vi) False Positive\n",
        "\n",
        "(vii) Type II Error\n",
        "\n",
        "(viii) Matthews correlation coefficient (MCC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwzgklanwZy6",
        "outputId": "c9154d30-ae98-42ff-cd89-9cb9aa63d53a"
      },
      "source": [
        "# Predicting of Test data of Filter method \n",
        "models = []\n",
        "models.append(('Keras Classifier - Filter', KerasClassifier(build_fn=create_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME1')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Model : \", name)\n",
        "  model_start = time.time()\n",
        "  model.fit(pca_filter_train_features, Y)\n",
        "  print(\"Time to build model (sec) : %.4f \" % round(time.time()-model_start,4))\n",
        "  start = time.time()\n",
        "  predicted = model.predict(pca_filter_test_features)\n",
        "  print(\"Time to test model (sec) : %.4f \" % round(time.time()-start,4))\n",
        "  matrix = confusion_matrix(Y_t, predicted)\n",
        "  print(\"Time elapsed (sec): %.4f \" % round(time.time()-model_start, 4))\n",
        "  print(matrix)\n",
        "\n",
        "  TN1 = matrix[0][0]\n",
        "  FN1 = matrix[1][0]\n",
        "  FP1 = matrix[0][1]\n",
        "  TP1 = matrix[1][1]\n",
        "\n",
        "  DetectionRate_LR = TP1/(TP1+FN1)\n",
        "  Alarm_LR =   FP1/(FP1+TN1)\n",
        "\n",
        "  # To built a MCC for LR\n",
        "  MCC_num_LR= (TP1*TN1)-(FP1*FN1) \n",
        "  MCC_din_LR= math.sqrt((TP1 + FP1)*(TP1+FN1)*(TN1 + FP1)*(TN1+FN1))\n",
        "\n",
        "  MCC_LR = MCC_num_LR / MCC_din_LR\n",
        "  Acc_LR = (TP1 + TN1) / (TP1+FP1+FN1+TN1)\n",
        "  Err_LR = 1 - Acc_LR\n",
        "  T2_Err = (FN1 / (FN1+TP1))\n",
        "  print(\"Model Accuracy : %s\"%(Acc_LR))\n",
        "  print(\"Model Error Rate : %s\"%(Err_LR))\n",
        "  print(\"Detection Rate :%s\"%(DetectionRate_LR))\n",
        "  print(\"False Positive Type 1 :%s\"%(Alarm_LR))\n",
        "  print(\"Type 2 Error : %s\"%(T2_Err))\n",
        "  print(\"Matthews correlation coefficient (MCC):%s\"%(MCC_LR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Model :  Keras Classifier - Filter\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.2341 - accuracy: 0.7492\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1627 - accuracy: 0.7964\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1570 - accuracy: 0.7966\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1551 - accuracy: 0.7962\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1494 - accuracy: 0.8039\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1478 - accuracy: 0.8065\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1483 - accuracy: 0.8009\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1457 - accuracy: 0.8061\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1429 - accuracy: 0.8112\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1430 - accuracy: 0.8099\n",
            "Time to build model (sec) : 17.6510 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test model (sec) : 0.4761 \n",
            "Time elapsed (sec): 18.1435 \n",
            "[[10631   729]\n",
            " [ 2161  1539]]\n",
            "Model Accuracy : 0.8081009296148738\n",
            "Model Error Rate : 0.1918990703851262\n",
            "Detection Rate :0.41594594594594597\n",
            "False Positive Type 1 :0.0641725352112676\n",
            "Type 2 Error : 0.5840540540540541\n",
            "Matthews correlation coefficient (MCC):0.42341114652864553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoMw1fYSSmfX",
        "outputId": "cdf4f77b-2ba7-4366-a05e-63eb36e1e09c"
      },
      "source": [
        "# Predicting of Test data of Wrapper method \n",
        "models = []\n",
        "models.append(('Keras Classifier - Wrapper', KerasClassifier(build_fn=create_model6, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME4')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Model : \", name)\n",
        "  model_start = time.time()\n",
        "  model.fit(pca_wrapper_train_features, Y)\n",
        "  print(\"Time to build model (sec) : %.4f \" % round(time.time()-model_start,4))\n",
        "  start = time.time()\n",
        "  predicted = model.predict(pca_wrapper_test_features)\n",
        "  print(\"Time to test model (sec) : %.4f \" % round(time.time()-start,4))\n",
        "  matrix = confusion_matrix(Y_t, predicted)\n",
        "  print(\"Time elapsed (sec): %.4f \" % round(time.time()-model_start, 4))\n",
        "  print(matrix)\n",
        "\n",
        "  TN1 = matrix[0][0]\n",
        "  FN1 = matrix[1][0]\n",
        "  FP1 = matrix[0][1]\n",
        "  TP1 = matrix[1][1]\n",
        "\n",
        "  DetectionRate_LR = TP1/(TP1+FN1)\n",
        "  Alarm_LR =   FP1/(FP1+TN1)\n",
        "\n",
        "  # To built a MCC for LR\n",
        "  MCC_num_LR= (TP1*TN1)-(FP1*FN1) \n",
        "  MCC_din_LR= math.sqrt((TP1 + FP1)*(TP1+FN1)*(TN1 + FP1)*(TN1+FN1))\n",
        "\n",
        "  MCC_LR = MCC_num_LR / MCC_din_LR\n",
        "  Acc_LR = (TP1 + TN1) / (TP1+FP1+FN1+TN1)\n",
        "  Err_LR = 1 - Acc_LR\n",
        "  T2_Err = (FN1 / (FN1+TP1))\n",
        "  print(\"Model Accuracy : %s\"%(Acc_LR))\n",
        "  print(\"Model Error Rate : %s\"%(Err_LR))\n",
        "  print(\"Detection Rate :%s\"%(DetectionRate_LR))\n",
        "  print(\"False Positive Type 1 :%s\"%(Alarm_LR))\n",
        "  print(\"Type 2 Error : %s\"%(T2_Err))\n",
        "  print(\"Matthews correlation coefficient (MCC):%s\"%(MCC_LR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Model :  Keras Classifier - Wrapper\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.2256 - accuracy: 0.7534\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1613 - accuracy: 0.8051\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1530 - accuracy: 0.8225\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1511 - accuracy: 0.8188\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1439 - accuracy: 0.8289\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1434 - accuracy: 0.8280\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1421 - accuracy: 0.8257\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1404 - accuracy: 0.8286\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1397 - accuracy: 0.8277\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1388 - accuracy: 0.8252\n",
            "Time to build model (sec) : 17.2878 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test model (sec) : 0.4686 \n",
            "Time elapsed (sec): 17.7718 \n",
            "[[10644   716]\n",
            " [ 1893  1807]]\n",
            "Model Accuracy : 0.8267596281540505\n",
            "Model Error Rate : 0.17324037184594954\n",
            "Detection Rate :0.4883783783783784\n",
            "False Positive Type 1 :0.0630281690140845\n",
            "Type 2 Error : 0.5116216216216216\n",
            "Matthews correlation coefficient (MCC):0.49032184264367223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEympxhUTt3t",
        "outputId": "63cfa9d1-d2a8-47cc-b9ce-d378aee2e534"
      },
      "source": [
        "# Predicting of Test data of Embedded method \n",
        "models = []\n",
        "models.append(('Keras Classifier - Embedded', KerasClassifier(build_fn=create_model7, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='adam')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Model : \", name)\n",
        "  model_start = time.time()\n",
        "  model.fit(pca_embedded_train_features, Y)\n",
        "  print(\"Time to build model (sec) : %.4f \" % round(time.time()-model_start,4))\n",
        "  start = time.time()\n",
        "  predicted = model.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test model (sec) : %.4f \" % round(time.time()-start,4))\n",
        "  matrix = confusion_matrix(Y_t, predicted)\n",
        "  print(\"Time elapsed (sec): %.4f \" % round(time.time()-model_start, 4))\n",
        "  print(matrix)\n",
        "\n",
        "  TN1 = matrix[0][0]\n",
        "  FN1 = matrix[1][0]\n",
        "  FP1 = matrix[0][1]\n",
        "  TP1 = matrix[1][1]\n",
        "\n",
        "  DetectionRate_LR = TP1/(TP1+FN1)\n",
        "  Alarm_LR =   FP1/(FP1+TN1)\n",
        "\n",
        "  # To built a MCC for LR\n",
        "  MCC_num_LR= (TP1*TN1)-(FP1*FN1) \n",
        "  MCC_din_LR= math.sqrt((TP1 + FP1)*(TP1+FN1)*(TN1 + FP1)*(TN1+FN1))\n",
        "\n",
        "  MCC_LR = MCC_num_LR / MCC_din_LR\n",
        "  Acc_LR = (TP1 + TN1) / (TP1+FP1+FN1+TN1)\n",
        "  Err_LR = 1 - Acc_LR\n",
        "  T2_Err = (FN1 / (FN1+TP1))\n",
        "  print(\"Model Accuracy : %s\"%(Acc_LR))\n",
        "  print(\"Model Error Rate : %s\"%(Err_LR))\n",
        "  print(\"Detection Rate :%s\"%(DetectionRate_LR))\n",
        "  print(\"False Positive Type 1 :%s\"%(Alarm_LR))\n",
        "  print(\"Type 2 Error : %s\"%(T2_Err))\n",
        "  print(\"Matthews correlation coefficient (MCC):%s\"%(MCC_LR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Model :  Keras Classifier - Embedded\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.2352 - accuracy: 0.7770\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1648 - accuracy: 0.7925\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1588 - accuracy: 0.7993\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1575 - accuracy: 0.7994\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1523 - accuracy: 0.8074\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1519 - accuracy: 0.8073\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1489 - accuracy: 0.8130\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1464 - accuracy: 0.8163\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1455 - accuracy: 0.8159\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1462 - accuracy: 0.8151\n",
            "Time to build model (sec) : 17.4145 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test model (sec) : 0.4760 \n",
            "Time elapsed (sec): 17.9057 \n",
            "[[7899 3461]\n",
            " [1939 1761]]\n",
            "Model Accuracy : 0.6414342629482072\n",
            "Model Error Rate : 0.35856573705179284\n",
            "Detection Rate :0.47594594594594597\n",
            "False Positive Type 1 :0.3046654929577465\n",
            "Type 2 Error : 0.524054054054054\n",
            "Matthews correlation coefficient (MCC):0.1549265991905138\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}