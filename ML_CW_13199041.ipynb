{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_CW_13199041",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf5EUexKWLX2"
      },
      "source": [
        "#Loading the Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIdYxi-Re7ZT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import tensorflow as tf \n",
        "from pandas import read_csv\n",
        "from pandas import set_option\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers.core import Activation\n",
        "from keras.optimizers import Optimizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras import backend\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import kullback_leibler_divergence\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import RFE , chi2\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from scipy.stats import uniform\n",
        "from matplotlib import pyplot\n",
        "import random as python_random\n",
        "from numpy import reshape\n",
        "from numpy import set_printoptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUSkkNrBWV1D"
      },
      "source": [
        "#Dataset Loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW8OIK4ufjiD"
      },
      "source": [
        "# Read the data set \n",
        "filename1 = 'adult.data'\n",
        "filename2 = 'adult.test'\n",
        "names = ('age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','class')\n",
        "sample_train = pd.read_csv(filename1,names= names, na_values=[' ?','?'])\n",
        "sample_test = pd.read_csv(filename2, names= names, na_values=[' ?','?'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2Lq0g7xdQs",
        "outputId": "19c5cda1-7df6-4804-9fee-4ef7341c11e7"
      },
      "source": [
        "# To see the shape of the  train dataset\n",
        "sample_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32561, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5G45jf-0S5q",
        "outputId": "1eea5d04-0fa9-45a5-f774-cc1af5b550ae"
      },
      "source": [
        "# To see the shape of the  test dataset\n",
        "sample_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16282, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCV7pHohWiPB"
      },
      "source": [
        "#Cleaning the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiF0KreFxhwu",
        "outputId": "6de4fced-61bf-43b0-f444-9058aef792aa"
      },
      "source": [
        "# Here we drop the rows which has ' ?' in the test dataset\n",
        "data_train = sample_train.dropna()\n",
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCBkDiDH0JDV",
        "outputId": "52251138-38d5-497d-f9bd-c9c67825cf82"
      },
      "source": [
        "# Here we drop the rows which has ' ?' in the train dataset\n",
        "data_test = sample_test.dropna()\n",
        "data_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "pZjG-6bD5NSv",
        "outputId": "8f23b20c-e692-424e-f69a-2ec40d1d4fa9"
      },
      "source": [
        "data_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  fnlwgt  ... hours-per-week  native-country   class\n",
              "0       39          State-gov   77516  ...             40   United-States   <=50K\n",
              "1       50   Self-emp-not-inc   83311  ...             13   United-States   <=50K\n",
              "2       38            Private  215646  ...             40   United-States   <=50K\n",
              "3       53            Private  234721  ...             40   United-States   <=50K\n",
              "4       28            Private  338409  ...             40            Cuba   <=50K\n",
              "...    ...                ...     ...  ...            ...             ...     ...\n",
              "32556   27            Private  257302  ...             38   United-States   <=50K\n",
              "32557   40            Private  154374  ...             40   United-States    >50K\n",
              "32558   58            Private  151910  ...             40   United-States   <=50K\n",
              "32559   22            Private  201490  ...             20   United-States   <=50K\n",
              "32560   52       Self-emp-inc  287927  ...             40   United-States    >50K\n",
              "\n",
              "[30162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "ZMWwRfhJSpzl",
        "outputId": "5ca3e4fc-37c2-4dc5-952d-927c71a76a78"
      },
      "source": [
        "data_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802.0</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814.0</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951.0</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323.0</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>34</td>\n",
              "      <td>Private</td>\n",
              "      <td>198693.0</td>\n",
              "      <td>10th</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>33</td>\n",
              "      <td>Private</td>\n",
              "      <td>245211.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>39</td>\n",
              "      <td>Private</td>\n",
              "      <td>215419.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>374983.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>83891.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Asian-Pac-Islander</td>\n",
              "      <td>Male</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>35</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>182148.0</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age      workclass    fnlwgt  ... hours-per-week  native-country    class\n",
              "1      25        Private  226802.0  ...           40.0   United-States   <=50K.\n",
              "2      38        Private   89814.0  ...           50.0   United-States   <=50K.\n",
              "3      28      Local-gov  336951.0  ...           40.0   United-States    >50K.\n",
              "4      44        Private  160323.0  ...           40.0   United-States    >50K.\n",
              "6      34        Private  198693.0  ...           30.0   United-States   <=50K.\n",
              "...    ..            ...       ...  ...            ...             ...      ...\n",
              "16276  33        Private  245211.0  ...           40.0   United-States   <=50K.\n",
              "16277  39        Private  215419.0  ...           36.0   United-States   <=50K.\n",
              "16279  38        Private  374983.0  ...           50.0   United-States   <=50K.\n",
              "16280  44        Private   83891.0  ...           40.0   United-States   <=50K.\n",
              "16281  35   Self-emp-inc  182148.0  ...           60.0   United-States    >50K.\n",
              "\n",
              "[15060 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE1WejYQWq72"
      },
      "source": [
        "## Checking if there are any ' ?' or null values in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSJ3iQEZ0oNi",
        "outputId": "31636e9a-a047-4d46-9ae7-42c4aeebd030"
      },
      "source": [
        "# Here we check if there are any '?' values in any of the features\n",
        "for c in names:\n",
        "  print(c)\n",
        "  print(data_train[c].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "36    852\n",
            "31    851\n",
            "33    837\n",
            "34    836\n",
            "35    828\n",
            "     ... \n",
            "82      7\n",
            "83      5\n",
            "88      3\n",
            "85      3\n",
            "86      1\n",
            "Name: age, Length: 72, dtype: int64\n",
            "workclass\n",
            " Private             22286\n",
            " Self-emp-not-inc     2499\n",
            " Local-gov            2067\n",
            " State-gov            1279\n",
            " Self-emp-inc         1074\n",
            " Federal-gov           943\n",
            " Without-pay            14\n",
            "Name: workclass, dtype: int64\n",
            "fnlwgt\n",
            "203488    13\n",
            "113364    12\n",
            "164190    12\n",
            "123011    12\n",
            "148995    12\n",
            "          ..\n",
            "34393      1\n",
            "288341     1\n",
            "239415     1\n",
            "118352     1\n",
            "229376     1\n",
            "Name: fnlwgt, Length: 20263, dtype: int64\n",
            "education\n",
            " HS-grad         9840\n",
            " Some-college    6678\n",
            " Bachelors       5044\n",
            " Masters         1627\n",
            " Assoc-voc       1307\n",
            " 11th            1048\n",
            " Assoc-acdm      1008\n",
            " 10th             820\n",
            " 7th-8th          557\n",
            " Prof-school      542\n",
            " 9th              455\n",
            " 12th             377\n",
            " Doctorate        375\n",
            " 5th-6th          288\n",
            " 1st-4th          151\n",
            " Preschool         45\n",
            "Name: education, dtype: int64\n",
            "education-num\n",
            "9     9840\n",
            "10    6678\n",
            "13    5044\n",
            "14    1627\n",
            "11    1307\n",
            "7     1048\n",
            "12    1008\n",
            "6      820\n",
            "4      557\n",
            "15     542\n",
            "5      455\n",
            "8      377\n",
            "16     375\n",
            "3      288\n",
            "2      151\n",
            "1       45\n",
            "Name: education-num, dtype: int64\n",
            "marital-status\n",
            " Married-civ-spouse       14065\n",
            " Never-married             9726\n",
            " Divorced                  4214\n",
            " Separated                  939\n",
            " Widowed                    827\n",
            " Married-spouse-absent      370\n",
            " Married-AF-spouse           21\n",
            "Name: marital-status, dtype: int64\n",
            "occupation\n",
            " Prof-specialty       4038\n",
            " Craft-repair         4030\n",
            " Exec-managerial      3992\n",
            " Adm-clerical         3721\n",
            " Sales                3584\n",
            " Other-service        3212\n",
            " Machine-op-inspct    1966\n",
            " Transport-moving     1572\n",
            " Handlers-cleaners    1350\n",
            " Farming-fishing       989\n",
            " Tech-support          912\n",
            " Protective-serv       644\n",
            " Priv-house-serv       143\n",
            " Armed-Forces            9\n",
            "Name: occupation, dtype: int64\n",
            "relationship\n",
            " Husband           12463\n",
            " Not-in-family      7726\n",
            " Own-child          4466\n",
            " Unmarried          3212\n",
            " Wife               1406\n",
            " Other-relative      889\n",
            "Name: relationship, dtype: int64\n",
            "race\n",
            " White                 25933\n",
            " Black                  2817\n",
            " Asian-Pac-Islander      895\n",
            " Amer-Indian-Eskimo      286\n",
            " Other                   231\n",
            "Name: race, dtype: int64\n",
            "sex\n",
            " Male      20380\n",
            " Female     9782\n",
            "Name: sex, dtype: int64\n",
            "capital-gain\n",
            "0        27624\n",
            "15024      337\n",
            "7688       270\n",
            "7298       240\n",
            "99999      148\n",
            "         ...  \n",
            "401          1\n",
            "22040        1\n",
            "4931         1\n",
            "1455         1\n",
            "1639         1\n",
            "Name: capital-gain, Length: 118, dtype: int64\n",
            "capital-loss\n",
            "0       28735\n",
            "1902      194\n",
            "1977      162\n",
            "1887      155\n",
            "1848       50\n",
            "        ...  \n",
            "419         1\n",
            "1411        1\n",
            "1539        1\n",
            "2472        1\n",
            "2467        1\n",
            "Name: capital-loss, Length: 90, dtype: int64\n",
            "hours-per-week\n",
            "40    14251\n",
            "50     2718\n",
            "45     1753\n",
            "60     1405\n",
            "35     1184\n",
            "      ...  \n",
            "94        1\n",
            "87        1\n",
            "74        1\n",
            "82        1\n",
            "92        1\n",
            "Name: hours-per-week, Length: 94, dtype: int64\n",
            "native-country\n",
            " United-States                 27504\n",
            " Mexico                          610\n",
            " Philippines                     188\n",
            " Germany                         128\n",
            " Puerto-Rico                     109\n",
            " Canada                          107\n",
            " El-Salvador                     100\n",
            " India                           100\n",
            " Cuba                             92\n",
            " England                          86\n",
            " Jamaica                          80\n",
            " South                            71\n",
            " China                            68\n",
            " Italy                            68\n",
            " Dominican-Republic               67\n",
            " Vietnam                          64\n",
            " Guatemala                        63\n",
            " Japan                            59\n",
            " Columbia                         56\n",
            " Poland                           56\n",
            " Haiti                            42\n",
            " Taiwan                           42\n",
            " Iran                             42\n",
            " Portugal                         34\n",
            " Nicaragua                        33\n",
            " Peru                             30\n",
            " Greece                           29\n",
            " Ecuador                          27\n",
            " France                           27\n",
            " Ireland                          24\n",
            " Hong                             19\n",
            " Cambodia                         18\n",
            " Trinadad&Tobago                  18\n",
            " Laos                             17\n",
            " Thailand                         17\n",
            " Yugoslavia                       16\n",
            " Outlying-US(Guam-USVI-etc)       14\n",
            " Hungary                          13\n",
            " Honduras                         12\n",
            " Scotland                         11\n",
            " Holand-Netherlands                1\n",
            "Name: native-country, dtype: int64\n",
            "class\n",
            " <=50K    22654\n",
            " >50K      7508\n",
            "Name: class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfouFHiSS7Du",
        "outputId": "50c81ef1-6b09-4785-945f-d90c7a0ef4ba"
      },
      "source": [
        "for c in names:\n",
        "  print(c)\n",
        "  print(data_test[c].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age\n",
            "35    444\n",
            "33    442\n",
            "36    431\n",
            "31    423\n",
            "38    420\n",
            "     ... \n",
            "85      2\n",
            "88      2\n",
            "87      1\n",
            "89      1\n",
            "84      1\n",
            "Name: age, Length: 73, dtype: int64\n",
            "workclass\n",
            " Private             11021\n",
            " Self-emp-not-inc     1297\n",
            " Local-gov            1033\n",
            " State-gov             667\n",
            " Self-emp-inc          572\n",
            " Federal-gov           463\n",
            " Without-pay             7\n",
            "Name: workclass, dtype: int64\n",
            "fnlwgt\n",
            "136986.0    9\n",
            "203488.0    8\n",
            "127651.0    8\n",
            "120277.0    8\n",
            "125892.0    8\n",
            "           ..\n",
            "153813.0    1\n",
            "242589.0    1\n",
            "227540.0    1\n",
            "77143.0     1\n",
            "204244.0    1\n",
            "Name: fnlwgt, Length: 11913, dtype: int64\n",
            "education\n",
            " HS-grad         4943\n",
            " Some-college    3221\n",
            " Bachelors       2526\n",
            " Masters          887\n",
            " Assoc-voc        652\n",
            " 11th             571\n",
            " Assoc-acdm       499\n",
            " 10th             403\n",
            " 7th-8th          266\n",
            " Prof-school      243\n",
            " 9th              221\n",
            " 12th             200\n",
            " Doctorate        169\n",
            " 5th-6th          161\n",
            " 1st-4th           71\n",
            " Preschool         27\n",
            "Name: education, dtype: int64\n",
            "education-num\n",
            "9.0     4943\n",
            "10.0    3221\n",
            "13.0    2526\n",
            "14.0     887\n",
            "11.0     652\n",
            "7.0      571\n",
            "12.0     499\n",
            "6.0      403\n",
            "4.0      266\n",
            "15.0     243\n",
            "5.0      221\n",
            "8.0      200\n",
            "16.0     169\n",
            "3.0      161\n",
            "2.0       71\n",
            "1.0       27\n",
            "Name: education-num, dtype: int64\n",
            "marital-status\n",
            " Married-civ-spouse       6990\n",
            " Never-married            4872\n",
            " Divorced                 2083\n",
            " Separated                 472\n",
            " Widowed                   450\n",
            " Married-spouse-absent     182\n",
            " Married-AF-spouse          11\n",
            "Name: marital-status, dtype: int64\n",
            "occupation\n",
            " Exec-managerial      1992\n",
            " Craft-repair         1990\n",
            " Prof-specialty       1970\n",
            " Sales                1824\n",
            " Adm-clerical         1819\n",
            " Other-service        1596\n",
            " Machine-op-inspct    1004\n",
            " Transport-moving      744\n",
            " Handlers-cleaners     696\n",
            " Tech-support          508\n",
            " Farming-fishing       491\n",
            " Protective-serv       332\n",
            " Priv-house-serv        89\n",
            " Armed-Forces            5\n",
            "Name: occupation, dtype: int64\n",
            "relationship\n",
            " Husband           6203\n",
            " Not-in-family     3976\n",
            " Own-child         2160\n",
            " Unmarried         1576\n",
            " Wife               685\n",
            " Other-relative     460\n",
            "Name: relationship, dtype: int64\n",
            "race\n",
            " White                 12970\n",
            " Black                  1411\n",
            " Asian-Pac-Islander      408\n",
            " Amer-Indian-Eskimo      149\n",
            " Other                   122\n",
            "Name: race, dtype: int64\n",
            "sex\n",
            " Male      10147\n",
            " Female     4913\n",
            "Name: sex, dtype: int64\n",
            "capital-gain\n",
            "0.0        13808\n",
            "15024.0      161\n",
            "7688.0       121\n",
            "7298.0       111\n",
            "99999.0       81\n",
            "           ...  \n",
            "41310.0        1\n",
            "991.0          1\n",
            "1173.0         1\n",
            "34095.0        1\n",
            "2062.0         1\n",
            "Name: capital-gain, Length: 110, dtype: int64\n",
            "capital-loss\n",
            "0.0       14347\n",
            "1902.0      100\n",
            "1977.0       84\n",
            "1887.0       73\n",
            "2415.0       23\n",
            "          ...  \n",
            "653.0         1\n",
            "1911.0        1\n",
            "2547.0        1\n",
            "2282.0        1\n",
            "2603.0        1\n",
            "Name: capital-loss, Length: 79, dtype: int64\n",
            "hours-per-week\n",
            "40.0    7107\n",
            "50.0    1376\n",
            "45.0     849\n",
            "60.0     680\n",
            "35.0     592\n",
            "        ... \n",
            "89.0       1\n",
            "69.0       1\n",
            "79.0       1\n",
            "76.0       1\n",
            "73.0       1\n",
            "Name: hours-per-week, Length: 89, dtype: int64\n",
            "native-country\n",
            " United-States                 13788\n",
            " Mexico                          293\n",
            " Philippines                      95\n",
            " Puerto-Rico                      66\n",
            " Germany                          65\n",
            " Canada                           56\n",
            " India                            47\n",
            " El-Salvador                      47\n",
            " China                            45\n",
            " Cuba                             41\n",
            " England                          33\n",
            " Italy                            32\n",
            " Dominican-Republic               30\n",
            " Japan                            30\n",
            " South                            30\n",
            " Portugal                         28\n",
            " Haiti                            27\n",
            " Columbia                         26\n",
            " Poland                           25\n",
            " Guatemala                        23\n",
            " Jamaica                          23\n",
            " Greece                           20\n",
            " Vietnam                          19\n",
            " Ecuador                          16\n",
            " Nicaragua                        15\n",
            " Peru                             15\n",
            " Iran                             14\n",
            " Taiwan                           13\n",
            " Thailand                         12\n",
            " Ireland                          12\n",
            " Hong                              9\n",
            " France                            9\n",
            " Scotland                          9\n",
            " Cambodia                          8\n",
            " Outlying-US(Guam-USVI-etc)        8\n",
            " Trinadad&Tobago                   8\n",
            " Yugoslavia                        7\n",
            " Honduras                          7\n",
            " Hungary                           5\n",
            " Laos                              4\n",
            "Name: native-country, dtype: int64\n",
            "class\n",
            " <=50K.    11360\n",
            " >50K.      3700\n",
            "Name: class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-1k3FsXXI5O"
      },
      "source": [
        "# Data Conversion (using Leable Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMlY8DGWAAxl"
      },
      "source": [
        "# Using Lable Encoder on train data to convert the Categorical data into Numerical data\n",
        "data_train_new = data_train.apply(LabelEncoder().fit_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "KxGh35KKAKPe",
        "outputId": "6b057b9a-6b7f-4d09-a322-0eb451719283"
      },
      "source": [
        "data_train_new\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "      <td>2491</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>2727</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>13188</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>14354</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>18120</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>15471</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>7555</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>7377</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>12060</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>16689</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30162 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  fnlwgt  ...  hours-per-week  native-country  class\n",
              "0       22          5    2491  ...              39              38      0\n",
              "1       33          4    2727  ...              12              38      0\n",
              "2       21          2   13188  ...              39              38      0\n",
              "3       36          2   14354  ...              39              38      0\n",
              "4       11          2   18120  ...              39               4      0\n",
              "...    ...        ...     ...  ...             ...             ...    ...\n",
              "32556   10          2   15471  ...              37              38      0\n",
              "32557   23          2    7555  ...              39              38      1\n",
              "32558   41          2    7377  ...              39              38      0\n",
              "32559    5          2   12060  ...              19              38      0\n",
              "32560   35          3   16689  ...              39              38      1\n",
              "\n",
              "[30162 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY84JiCFTr7F"
      },
      "source": [
        "# Using Lable Encoder on train data to convert the Categorical data into Numerical data\n",
        "data_test_new = data_test.apply(LabelEncoder().fit_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "dYCHC5mHTyAN",
        "outputId": "8c87a5da-5290-4bdb-f1eb-9b4fd7ef4176"
      },
      "source": [
        "data_test_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>8315</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>1754</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>10750</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>4780</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>7091</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16276</th>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>8927</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16277</th>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>7893</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16279</th>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>11193</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16280</th>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>1593</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16281</th>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>6062</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>59</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15060 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  fnlwgt  ...  hours-per-week  native-country  class\n",
              "1        8          2    8315  ...              39              37      0\n",
              "2       21          2    1754  ...              49              37      0\n",
              "3       11          1   10750  ...              39              37      1\n",
              "4       27          2    4780  ...              39              37      1\n",
              "6       17          2    7091  ...              29              37      0\n",
              "...    ...        ...     ...  ...             ...             ...    ...\n",
              "16276   16          2    8927  ...              39              37      0\n",
              "16277   22          2    7893  ...              35              37      0\n",
              "16279   21          2   11193  ...              49              37      0\n",
              "16280   27          2    1593  ...              39              37      0\n",
              "16281   18          3    6062  ...              59              37      1\n",
              "\n",
              "[15060 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCpMyMWRT-r7"
      },
      "source": [
        "#Splitting train data into array and using MinMaxScaler to scale the data\n",
        " & \n",
        "Splitting test data into array and using MinMaxScaler to scale the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugv3BRf4CN5c"
      },
      "source": [
        "# Training data \n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "array = data_train_new.values\n",
        "# Seperate array into input and output\n",
        "X = array[:, 0:14]\n",
        "Y = array[:, 14]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rX = scaler.fit_transform(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXfV2qFtUY-q"
      },
      "source": [
        "# FOR Test data\n",
        "array = data_test_new.values\n",
        "# Seperate array into input and output\n",
        "X_t = array[:, 0:14]\n",
        "Y_t = array[:, 14]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rX_t = scaler.fit_transform(X_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BztbWFaBXcaH"
      },
      "source": [
        "# Feature Selection (Filter, Wrapper & Embedded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3d04GDPWcbv"
      },
      "source": [
        "##Filter Method - SelectKBest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNMMOjJLIHR5"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "# feature extraction\n",
        "#Select k best on train set \n",
        "selectModel = SelectKBest(score_func=chi2, k=9)\n",
        "kBesttrain = selectModel.fit(rX,Y)\n",
        "selectTrainFeatures = kBesttrain.transform(rX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NKHtafhV7Lx",
        "outputId": "39e3ba2b-22ee-4be9-9d1d-02ef5528a638"
      },
      "source": [
        "selectTrainFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwk81Ua6VlLJ"
      },
      "source": [
        "#Select k best on test set \n",
        "kBesttest = selectModel.fit(rX_t,Y_t)\n",
        "selectTestFeatures = kBesttest.transform(rX_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2GU_XTgWAaD",
        "outputId": "ec1113ea-eac4-47eb-c671-9cabb4daa370"
      },
      "source": [
        "selectTestFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6hfGgKqV3Qo"
      },
      "source": [
        "##Wrapper Method - Recurssive Feature Elemination (RFE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yoDwiszXBmJ"
      },
      "source": [
        "#RFE on train set\n",
        "model_rfe = LogisticRegression(solver='liblinear') \n",
        "rfeModel = RFE(model_rfe, 9) \n",
        "rfeTrain = rfeModel.fit(rX,Y)\n",
        "rfeTrainFeatures = rfeTrain.transform(rX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS9r8X4RXQ_m",
        "outputId": "6cd5b995-798d-4c8c-913d-5ac240e2b21f"
      },
      "source": [
        "rfeTrainFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6I0_fbPXZGT"
      },
      "source": [
        "#RFE on test set\n",
        "rfeTest = rfeModel.fit(rX_t,Y_t)\n",
        "rfeTestFeatures = rfeTest.transform(rX_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNf3NitLXlUX",
        "outputId": "07e0c2a1-6f8e-4149-8890-24b6a4682079"
      },
      "source": [
        "rfeTestFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfcTjYEXYpkL"
      },
      "source": [
        "##Embedded method - using ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiG2SdaVYuHn"
      },
      "source": [
        "#ExtraTree on train set\n",
        "extraTreeModel = ExtraTreesClassifier(n_estimators=9)\n",
        "exTreeTrain = extraTreeModel.fit(rX, Y)\n",
        "clf_model = SelectFromModel(exTreeTrain, prefit=True)\n",
        "exTreeTrainFeatures = clf_model.transform(rX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO1R9zvBY_oL",
        "outputId": "72247445-283a-4999-e3a8-104405979724"
      },
      "source": [
        "exTreeTrainFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmMMnbNRj1P0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcQSTfe9Zgv2"
      },
      "source": [
        "#ExtraTree on test set\n",
        "exTreeTest = extraTreeModel.fit(rX_t, Y_t)\n",
        "clf_modeltest = SelectFromModel(exTreeTest, prefit=True)\n",
        "exTreeTestFeatures = clf_modeltest.transform(rX_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcVknE8lZnM8",
        "outputId": "cc4c9302-bcbf-494f-d734-7e1362d543d3"
      },
      "source": [
        "exTreeTestFeatures.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJOAa6HX2Eg"
      },
      "source": [
        "#Feature Elemination "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mLow_knXrZ6"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Filter (SelectKBest), Wrapper (RFE) & Embedded (ExtraTreesClassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYdy-1RfcfS_"
      },
      "source": [
        "###PCA -Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IBh5Rk9Zy19"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Filter (SelectKBest)\n",
        "pca = PCA(n_components=5, random_state = 7) \n",
        "fit_pca_train = pca.fit(selectTrainFeatures) \n",
        "pca_filter_train_features = fit_pca_train.transform(selectTrainFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKAbaXFra-uF"
      },
      "source": [
        "## On selected test feature dataset for PCA # \n",
        "fit_pca_test = pca.fit(selectTestFeatures) \n",
        "pca_filter_test_features = fit_pca_test.transform(selectTestFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR3o0OkcbBCc",
        "outputId": "649f02f0-2e59-4172-cd2c-b2cbb97f4a80"
      },
      "source": [
        "pca_filter_train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcc8tv3tbIeB",
        "outputId": "58092506-950c-4248-f430-f0b3c2678a45"
      },
      "source": [
        "pca_filter_test_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8QR-XHOci66"
      },
      "source": [
        "###PCA -Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7WyY-9bRSx"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Wrapper (RFE) \n",
        "fit_pca_train = pca.fit(rfeTrainFeatures) \n",
        "pca_wrapper_train_features = fit_pca_train.transform(rfeTrainFeatures)\n",
        "fit_pca_test = pca.fit(rfeTestFeatures) \n",
        "pca_wrapper_test_features = fit_pca_test.transform(rfeTestFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpH0xsO5cVBh",
        "outputId": "b6d58c9e-5833-49fe-e611-0f774b07248b"
      },
      "source": [
        "pca_wrapper_train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GdP3yLHcX_P",
        "outputId": "c138139e-4502-4b63-d302-3bf26d302ab6"
      },
      "source": [
        "pca_wrapper_test_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eaLw7jrcnre"
      },
      "source": [
        "###PCA - Embedded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2ca12Wecs-u"
      },
      "source": [
        "##Principal component Analysis (PCA) from the output of Wrapper (RFE) \n",
        "pca = PCA(n_components=5, random_state = 2) \n",
        "fit_pca_train = pca.fit(exTreeTrainFeatures) \n",
        "pca_embedded_train_features = fit_pca_train.transform(exTreeTrainFeatures)\n",
        "fit_pca_test = pca.fit(exTreeTestFeatures) \n",
        "pca_embedded_test_features = fit_pca_test.transform(exTreeTestFeatures)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz1bwS2nft1O",
        "outputId": "c552a646-1aa9-4079-a204-19b23b57d980"
      },
      "source": [
        "pca_embedded_train_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30162, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn7iv6c3fzyC",
        "outputId": "7150dfda-9e21-4473-867f-6d07255f4dfe"
      },
      "source": [
        "pca_embedded_test_features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15060, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0u5B0Xf4cl"
      },
      "source": [
        "#Using Neural Network Model on the PCA output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RduO9SkThMjG"
      },
      "source": [
        "##Training with PCA - FILTER METHOD output \"pca_filter_train_features \" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaLIsicrgCrN",
        "outputId": "df0a432c-7bda-4508-e14f-0108de2ec831"
      },
      "source": [
        "# Parameters Used\n",
        "optimizers = ['adam', 'rmsprop']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "\n",
        "def create_model1(optimizer=optimizers, init=inits):\n",
        "  # create model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu')) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model1, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_filter_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.831212 using {'batch_size': 20, 'epochs': 10, 'init': 'uniform', 'optimizer': 'rmsprop'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rc-EAeRhqws"
      },
      "source": [
        "##Training with PCA - WRPPER METHOD output \"pca_wrapper_filter_features\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muyaJG7MiBvN",
        "outputId": "9ef2fe0c-5179-457e-8e5f-cadcc01f60c0"
      },
      "source": [
        "# Parameters Used\n",
        "optimizers = ['adam', 'rmsprop']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model2(optimizer=optimizers, init=inits):\n",
        "  # create model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu')) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model2, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_wrapper_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.838704 using {'batch_size': 20, 'epochs': 10, 'init': 'uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK1C3kI8ZCZ7"
      },
      "source": [
        "##Training with PCA - EMBEDDED METHOD output \"pca_embedded_train_features\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf6q3KamZRHr",
        "outputId": "7cc229f9-916e-46d5-b463-8b5bceebebfb"
      },
      "source": [
        "# Parameters Used\n",
        "optimizers = ['adam', 'rmsprop']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model3(optimizer=optimizers, init=inits):\n",
        "  # create model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu')) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model3, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.832240 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFndcobgUkxa"
      },
      "source": [
        "#Creating WAME algorithm  with changing Learning Rate  so we create WAME1, WAME2, WAME3, WAME4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiLiB_iPfyiU"
      },
      "source": [
        "##WAME 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4aVWA6eUjGC"
      },
      "source": [
        "class WAME1(Optimizer):\n",
        " # Using Learning rate, alpha, epsilon, decay, eta+, eta-, zeta (min), zeat(max), zeta, eta\n",
        "   def __init__(self, learning_rate=0.001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME1, self).__init__(**kwargs) # This function call the init to read and assign the values \n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): # This function has all steps from  4 and 12 mentioned in the Mosca [1] paper and it denotes the for loop in the paper\n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       # This defines the step 4 of the algorithm\n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " # Here we can see the weights are initilised with the backend function of keras to define the layers\n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g # This is the multiplication condition on line number 4 and 6 of the algorithm\n",
        "           change_below_zero = K.less(change,0.) # This is a if statement mentioned on line 4 of the algorithm\n",
        "           change_above_zero = K.greater(change,0.) # This is a if statement mentioned on line 6 of the algorithm\n",
        "           zeeta = K.switch( #Switch is used instead of if else statement and it is in a loop in loop configuration\n",
        "               change_below_zero, # This is the first looped statement (i.e. line 4 of algorithm)\n",
        "               a * self.eta_minus, # This is the  line 7 of algorithm\n",
        "               K.switch(change_above_zero,  # This is the  else line 4 if true (loop in loop statement)\n",
        "                        a * self.eta_plus, # This is the else statement in line 5\n",
        "                        a) \n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) #This statement denoted end of line 5 and 7 (min and max) of the algorithm\n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) #This statement denoted end of line 9 (the else condition) of the algorithm\n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped #This statement denoted end of line 10 (the else condition) of the algorithm\n",
        "           a_rate = a_clipped / am_t #am_t == θij(t) # This statement denoted end of line 11 (the else condition) of the algorithm\n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) #This statement denoted end of line 12 (the else condition) of the algorithm\n",
        " \n",
        "           new_p = p_t\n",
        " # These statement updated the all the weights  and ready for use\n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME1, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwEvCLf-f3V1"
      },
      "source": [
        "##WAME 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woOJNepbeXOr"
      },
      "source": [
        "# For code nerration please refer to WAME1 above\n",
        "class WAME2(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.0001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME2, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): \n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       \n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g \n",
        "           change_below_zero = K.less(change,0.) \n",
        "           change_above_zero = K.greater(change,0.) \n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, \n",
        "               a * self.eta_minus, \n",
        "               K.switch(change_above_zero,  \n",
        "                        a * self.eta_plus, \n",
        "                        a)\n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) \n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) \n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped \n",
        "           a_rate = a_clipped / am_t \n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) \n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME2, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzIQd2nFlk6D"
      },
      "source": [
        "##WAME 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h2YV1_JesrD"
      },
      "source": [
        "# For code nerration please refer to WAME1 above\n",
        "class WAME3(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.00001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME3, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): \n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "      \n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g \n",
        "           change_below_zero = K.less(change,0.) \n",
        "           change_above_zero = K.greater(change,0.) \n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, \n",
        "               a * self.eta_minus, \n",
        "               K.switch(change_above_zero, \n",
        "                        a * self.eta_plus, \n",
        "                        a) \n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) \n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) \n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped \n",
        "           a_rate = a_clipped / am_t \n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) \n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME3, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2k0OICloNd"
      },
      "source": [
        "##WAME 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axb4iOiefHHU"
      },
      "source": [
        "# For code nerration please refer to WAME1 above\n",
        "class WAME4(Optimizer):\n",
        " \n",
        "   def __init__(self, learning_rate=0.000001, alpha = 0.9,\n",
        "                epsilon=1e-11, decay=0., eta_plus = 1.2, eta_minus = 0.1,\n",
        "                zeta_min=1e-2, zeta_max=1e2, zeta = 0, eta = 0,\n",
        "                **kwargs):\n",
        "      \n",
        "       super(WAME4, self).__init__(**kwargs)\n",
        "       self.__dict__.update(locals())\n",
        "       self.iterations = K.variable(0)\n",
        "       self.learning_rate = K.variable(learning_rate)\n",
        "       self.alpha = K.variable(alpha)\n",
        "       self.zeta = K.variable(zeta)\n",
        "       self.decay = K.variable(decay)\n",
        "       self.eta_plus = K.variable(eta_plus)\n",
        "       self.eta_minus = K.variable(eta_minus)\n",
        "       self.eta_min = eta_min\n",
        "       self.eta_max = eta_max\n",
        "       self.inital_decay = decay\n",
        "   \n",
        "   def get_updates(self, params, loss): \n",
        " \n",
        "       grads = self.get_gradients(loss, params)\n",
        "       self.updates = [K.update_add(self.iterations, 1)]\n",
        "       # 4\n",
        "       lr = self.learning_rate\n",
        "       if self.inital_decay > 0:\n",
        "           lr *= (1. / (1. + self.decay * self.iterations))\n",
        " \n",
        "       t = self.iterations + 1\n",
        " \n",
        "       shapes = [K.int_shape(p) for p in params]\n",
        "       prev_grads = [K.zeros(shape) for shape in shapes]\n",
        "       prev_param = [K.zeros(shape) for shape in shapes]\n",
        "       ms = [K.zeros(shape) for shape in shapes]\n",
        "       vs = [K.zeros(shape) for shape in shapes]\n",
        "       accs = [K.ones(shape) for shape in shapes]\n",
        "       acc_ms = [K.ones(shape) for shape in shapes]\n",
        "       acc_vs = [K.ones(shape) for shape in shapes]\n",
        "       self.weights = [self.iterations] + ms + vs\n",
        " \n",
        "       for p, g, m, v, a, am, av, pg, pp in zip(params, grads, ms, vs, accs,\n",
        "               acc_ms, acc_vs, prev_grads, prev_param):\n",
        " \n",
        "           change = pg * g \n",
        "           change_below_zero = K.less(change,0.) \n",
        "           change_above_zero = K.greater(change,0.) \n",
        "           zeeta = K.switch(\n",
        "               change_below_zero, \n",
        "               a * self.eta_minus,\n",
        "               K.switch(change_above_zero,  \n",
        "                        a * self.eta_plus, \n",
        "                        a) \n",
        "             \n",
        "           )\n",
        "           a_clipped = K.clip(zeeta, self.eta_min, self.eta_max) \n",
        "           v_t = (self.alpha * v) + (1. - self.alpha) * K.square(g) \n",
        "           am_t = (self.alpha * am) + (1. - self.alpha) * a_clipped \n",
        "           a_rate = a_clipped / am_t \n",
        "           p_t = p - lr * a_rate * g * (1/(K.sqrt(v_t + self.epsilon))) \n",
        " \n",
        "           new_p = p_t\n",
        " \n",
        "           self.updates.append(K.update(v, v_t))\n",
        "           self.updates.append(K.update(p, new_p))\n",
        "           self.updates.append(K.update(pg, p))\n",
        "           self.updates.append(K.update(a, zeeta))\n",
        "           self.updates.append(K.update(am, am_t))\n",
        "           self.updates.append(K.update(pp, p))\n",
        "       return self.updates\n",
        " \n",
        "   def get_config(self):\n",
        "       config = {'lr': float(K.get_value(self.lr)),\n",
        "                 'alpha': float(K.get_value(self.alpha)),\n",
        "                 'eta_plus': float(K.get_value(self.eta_plus)),\n",
        "                 'eta_minus': float(K.get_value(self.eta_minus)),\n",
        "                 'eta_min': float(self.eta_min),\n",
        "                 'eta_max': float(self.eta_max),\n",
        "                 'epsilon': self.epsilon}\n",
        "       base_config = super(WAME4, self).get_config()\n",
        "       return dict(list(base_config.items()) + list(config.items()))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eRe9CEEj1pA"
      },
      "source": [
        "#Configering The Keras Model For Filter, Wrapper and Embedded PCA Output and Geetting the best tune results in from Grid search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFSIoEQmpuUs"
      },
      "source": [
        "## Tuneing with PCA - FILTER METHOD output \"pca_filter_train_features \" with WAME 1, 2 , 3 & 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp73889fpUVE",
        "outputId": "ef433ec3-e3d6-422b-852f-4901e8b14075"
      },
      "source": [
        "#  Parameters Used\n",
        "optimizers = ['adam', 'rmsprop', 'WAME1', 'WAME2', 'WAME3', 'WAME4']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model4(optimizer=optimizers, init=inits):\n",
        "  # create model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu')) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model4, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_filter_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.831079 using {'batch_size': 20, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjDdADQJqIg_"
      },
      "source": [
        "##Tuneing with PCA - WRPPER METHOD output \"pca_wrapper_filter_features\" with WAME 1, 2, 3 & 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LZhbOKIqW_r",
        "outputId": "6ad86007-e3f7-4f97-c62f-459856af7a8b"
      },
      "source": [
        "# Parameters Used\n",
        "optimizers = ['adam', 'rmsprop', 'WAME1', 'WAME2', 'WAME3', 'WAME4']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model5(optimizer=optimizers, init=inits):\n",
        "  # create model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu')) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model5, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_wrapper_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.840992 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME4'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJVK9ziYq6R6"
      },
      "source": [
        "##Tuneing with PCA - EMBEDDED METHOD output \"pca_embedded_train_features\" with WAME 1, 2, 3 & 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPrUMdRpq6-H",
        "outputId": "f0504a5a-92cd-4a82-d146-b283e8203c86"
      },
      "source": [
        "# Parameters Used\n",
        "optimizers = ['adam', 'rmsprop', 'WAME1', 'WAME2', 'WAME3', 'WAME4']\n",
        "inits = ['uniform', 'glorot_uniform'] \n",
        "epochs = [5, 7, 10]\n",
        "batches = [20, 30, 40]\n",
        "\n",
        "def create_model6(optimizer=optimizers, init=inits):\n",
        "  # create model\n",
        "  mlp_model = Sequential()\n",
        "  mlp_model.add(Dense(12, input_dim= 5, activation='relu')) \n",
        "  mlp_model.add(Dense(10, activation='relu')) \n",
        "  mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Compile model\n",
        "  mlp_model.compile(loss = tf.keras.losses.mean_squared_error, metrics=[\"accuracy\"])\n",
        "  return mlp_model\n",
        "\n",
        "# create model\n",
        "model_keras = KerasClassifier(build_fn=create_model6, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "param_grid = dict(optimizer= optimizers, epochs=epochs, batch_size=batches, init=inits) \n",
        "grid = GridSearchCV(estimator=model_keras, param_grid=param_grid)\n",
        "grid_result = grid.fit(pca_embedded_train_features, Y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.833267 using {'batch_size': 40, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME3'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rjuVv1yr6q0"
      },
      "source": [
        "#Comparing best models (Betweent Filter, Wrapper & Embedded with WAME Output)  :\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHhZZkiFmVAU"
      },
      "source": [
        "##(a) Using Crossvalidation on hyperparameter tuned models of Keras Classifier (from output of Filter, Wrapper & Embedded) and compare the performance accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bmDJCdSPsbR9",
        "outputId": "29ee6852-cf6b-4ff0-a900-c38a1ef6203c"
      },
      "source": [
        "# The Keras on Filter output  \n",
        "# Best: 0.831079 using {'batch_size': 20, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME1'}\n",
        "models1 = []\n",
        "models1.append(('Keras Classifier - Filter ', KerasClassifier(build_fn=create_model4, batch_size= 20, epochs= 10, init = 'uniform', optimizer='WAME1')))\n",
        "# evaluate the model \n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model1 in models1:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True) # Using K Fold cross validation\n",
        "  cv_results = cross_val_score(model1, pca_filter_train_features, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm \n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1701 - accuracy: 0.7624\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1287 - accuracy: 0.8046\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.8138\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1190 - accuracy: 0.8210\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1187 - accuracy: 0.8258\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8237\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1171 - accuracy: 0.8287\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.8274\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8243\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1556 - accuracy: 0.7890\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1239 - accuracy: 0.8153\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1219 - accuracy: 0.8160\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.8233\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.8282\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1187 - accuracy: 0.8261\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8261\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1165 - accuracy: 0.8294\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1164 - accuracy: 0.8291\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8292\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1656 - accuracy: 0.7660\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1254 - accuracy: 0.8142\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1233 - accuracy: 0.8174\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.8267\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8271\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8319\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8302\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.8366\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8285\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8337\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1699 - accuracy: 0.7462\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1231 - accuracy: 0.8108\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.8132\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8227\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1197 - accuracy: 0.8238\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8259\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.8287\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8301\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8316\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8309\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1671 - accuracy: 0.7624\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1259 - accuracy: 0.8071\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1211 - accuracy: 0.8148\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.8140\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1179 - accuracy: 0.8220\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.8195\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1185 - accuracy: 0.8209\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8265\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.8233\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1169 - accuracy: 0.8250\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1636 - accuracy: 0.7780\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.8129\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1225 - accuracy: 0.8126\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1206 - accuracy: 0.8170\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1192 - accuracy: 0.8189\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1205 - accuracy: 0.8212\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8283\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1182 - accuracy: 0.8250\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8316\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1154 - accuracy: 0.8307\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1684 - accuracy: 0.7545\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1260 - accuracy: 0.8139\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1257 - accuracy: 0.8108\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1204 - accuracy: 0.8179\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.8208\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.8239\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1188 - accuracy: 0.8252\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1156 - accuracy: 0.8315\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8298\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8297\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1630 - accuracy: 0.7557\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1239 - accuracy: 0.8147\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.8171\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8226\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.8271\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.8265\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.8206\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1197 - accuracy: 0.8221\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1203 - accuracy: 0.8275\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.8255\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1832 - accuracy: 0.7412\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1277 - accuracy: 0.8126\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1206 - accuracy: 0.8190\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1191 - accuracy: 0.8211\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.8246\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1186 - accuracy: 0.8245\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.8278\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8286\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1195 - accuracy: 0.8251\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8288\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1627 - accuracy: 0.7546\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1204 - accuracy: 0.8224\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1197 - accuracy: 0.8250\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1192 - accuracy: 0.8276\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1186 - accuracy: 0.8281\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1189 - accuracy: 0.8249\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1194 - accuracy: 0.8253\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1174 - accuracy: 0.8299\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.8229\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8251\n",
            "Keras Classifier - Filter : 0.829520 (0.007528)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXk0lEQVR4nO3df7RdZX3n8feHIAQVAjRRkQDRgVEQKuoV6w/8hShNVXR0KQgCLhQdBTsWl8UWNaVjp84aa/2BOuDCVBQx2sGJSyxoB3WgaHMj4UegdCJFCPFHECwgggS/88feV4/XG+65yc1Nbp73a62z7tnPfvazn+cEzmfvZ59zdqoKSVJ7dtjaHZAkbR0GgCQ1ygCQpEYZAJLUKANAkhplAEhSowwATYskS5P81y3U9nFJLn2I9c9PsnZL7Hu2S/JnST61tfuhbZMBoClJ8s0kdybZeab2WVWfq6oXD/Shkuw/U/tP5+1Jrkvy8yRrk3wxySEz1YdNVVV/VVVv3Nr90LbJANDQkiwCDgcKePkM7XPHmdjPJD4M/DHwdmBP4D8CXwb+aGt2ajLbyGunbZgBoKk4AfgOsBQ48aEqJnlXkh8mWZfkjYNH7UnmJflMkvVJfpDkzCQ79OtOSnJFkg8l+SmwpC+7vF//7X4XVye5J8lrB/Z5epKf9Pt9w0D50iQfT/K1fpsrkjwmyd/2ZzP/kuQpGxnHAcDbgGOr6v9U1f1VdW9/VvLXUxzPz5LclORZffmtfX9PHNfXTyb5epK7k3wryX4D6z/cb3dXkpVJDh9YtyTJl5J8NsldwEl92Wf79XP7dT/t+7IiyaP7dY9NsjzJHUnWJHnTuHaX9WO8O8nqJCMP9e+v2cEA0FScAHyuf7xk7M1jvCRHAX8CvAjYH3j+uCofBeYBjwee17f7hoH1zwBuAh4NvH9ww6p6bv/0yVX1yKr6Qr/8mL7NvYGTgbOT7DGw6WuAM4H5wP3AlcD3+uUvAX+zkTEfAaytqn/eyPphx3MN8HvABcCFwNPpXpvjgY8leeRA/eOAv+z7toru9R6zAjiU7kzkAuCLSeYOrD+6H8/u47aDLrTnAfv0fXkL8It+3YXAWuCxwKuBv0rywoFtX97X2R1YDnzsIV4PzRIGgIaS5DnAfsCyqloJfB943Uaqvwb4dFWtrqp7gSUD7cwBjgHeXVV3V9XNwAeB1w9sv66qPlpVG6rqFwznAeCsqnqgqi4G7gGeMLD+oqpaWVX3ARcB91XVZ6rqQeALwIRnAHRvlD/c2E6HHM+/VdWnB/a1T9/X+6vqUuCXdGEw5qtV9e2quh/4c+CZSfYBqKrPVtVP+9fmg8DO48Z5ZVV9uap+NcFr90A/nv2r6sH+9birb/vZwJ9W1X1VtQr4FF2Qjbm8qi7ux3A+8OSNvSaaPQwADetE4NKqur1fvoCNTwM9Frh1YHnw+XzgYcAPBsp+QHfkPlH9Yf20qjYMLN8LDB5V/3jg+S8mWB6s+1vtAns9xH6HGc/4fVFVD7X/X4+/qu4B7qB7TUnyziQ3JPn3JD+jO6KfP9G2EzgfuAS4sJ+a++9JHta3fUdV3f0QY/jRwPN7gbleY5j9DABNKskudEf1z0vyoyQ/At4BPDnJREeCPwQWDizvM/D8droj0f0GyvYFbhtY3pZ+ovYfgYUPMec9zHim6tevVz81tCewrp/vfxfdv8UeVbU78O9ABrbd6GvXnx39RVUdBDwLeCndUf46YM8ku07jGDQLGAAaxiuAB4GD6OafDwUOBP4vvz1NMGYZ8IYkByZ5OPCesRX9FMIy4P1Jdu0vcP4J8Nkp9OfHdPPtW1xV/T/g48Dn033fYKf+YuoxSc6YpvGMtzjJc5LsRHct4DtVdSuwK7ABWA/smOS9wG7DNprkBUkO6aet7qILrl/1bf8T8N/6sf0+3XWUzRmDZgEDQMM4kW5O/5aq+tHYg+5C4HHjpwKq6mvAR4DLgDV0nxyC7uIrwGnAz+ku9F5ON5103hT6swT4u/6TLK/ZxDFNxdvpxno28DO66x+vBL7Sr9/c8Yx3AfA+uqmfp9FdKIZu+uYfgH+lm6K5j6lNlz2G7gLxXcANwLfopoUAjgUW0Z0NXAS8r6q+sRlj0CwQbwijLS3JgcB1wM7j5uk1TpKldJ86OnNr90XbP88AtEUkeWWSnfuPYn4A+Ipv/tK2xQDQlvJm4Cd00yUPAv9563ZH0nhOAUlSozwDkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmN2nHyKtuO+fPn16JFi7Z2NyRpVlm5cuXtVbVgfPmsCoBFixYxOjq6tbshSbNKkh9MVO4UkCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRs+qLYNJMSTIj+6mqGdmPNBEDQJrAVN+Yk/hmrlnHKSBJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqO8IYy2e3vuuSd33nnnFt/Plr6L2B577MEdd9yxRfehthgA2u7deeed28XdumbqNpVqh1NAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1aqgASHJUkhuTrElyxgTr901yWZKrklyTZHFffliSVf3j6iSvHNjm5iTX9utGp29IkqRhTPpFsCRzgLOBI4G1wIoky6vq+oFqZwLLquoTSQ4CLgYWAdcBI1W1IclewNVJvlJVG/rtXlBVt0/jeCRJQxrmDOAwYE1V3VRVvwQuBI4eV6eA3frn84B1AFV178Cb/dy+niRpGzBMAOwN3DqwvLYvG7QEOD7JWrqj/9PGViR5RpLVwLXAWwYCoYBLk6xMcsom9l+StImm6yLwscDSqloILAbOT7IDQFV9t6qeBDwdeHeSuf02z6mqpwJ/CLwtyXMnajjJKUlGk4yuX79+mrorSRomAG4D9hlYXtiXDToZWAZQVVfSTffMH6xQVTcA9wAH98u39X9/AlxEN9X0O6rqnKoaqaqRBQsWDNFdSdIwhgmAFcABSR6XZCfgGGD5uDq3AEcAJDmQLgDW99vs2JfvBzwRuDnJI5Ls2pc/Angx3QVjSdIMmfRTQP0neE4FLgHmAOdV1eokZwGjVbUcOB04N8k76Ob2T6qqSvIc4IwkDwC/At5aVbcneTxwUf/ztjsCF1TVP2yREUqSJpTZ9DvpIyMjNTrqVwY0NUm2m/sBbA/j0MxLsrKqRsaX+01gSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0aKgCSHJXkxiRrkpwxwfp9k1yW5Kok1yRZ3JcflmRV/7g6ySuHbVOStGXtOFmFJHOAs4EjgbXAiiTLq+r6gWpnAsuq6hNJDgIuBhYB1wEjVbUhyV7A1Um+AtQQbUqStqBhzgAOA9ZU1U1V9UvgQuDocXUK2K1/Pg9YB1BV91bVhr58bl9v2DYlSVvQMAGwN3DrwPLavmzQEuD4JGvpjv5PG1uR5BlJVgPXAm/pA2GYNse2PyXJaJLR9evXD9FdSdIwpusi8LHA0qpaCCwGzk+yA0BVfbeqngQ8HXh3krlTabiqzqmqkaoaWbBgwTR1V5I06TUA4DZgn4HlhX3ZoJOBowCq6sr+TX4+8JOxClV1Q5J7gIOHbFOaFvW+3WDJvK3djc1W79tt8krSFAwTACuAA5I8ju5N+hjgdePq3AIcASxNciDdfP/6fptb+4vA+wFPBG4GfjZEm9K0yF/cRVVNXnEbl4RasrV7oe3JpAHQv3mfClwCzAHOq6rVSc4CRqtqOXA6cG6Sd9Bd6D2pqirJc4AzkjwA/Ap4a1XdDjBRm1tigJKkiWU2HRmNjIzU6Ojo1u6GZpkk288ZwHYwDs28JCuramR8ud8ElqRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRg0VAEmOSnJjkjVJzphg/b5JLktyVZJrkizuy49MsjLJtf3fFw5s882+zVX941HTNyxJ0mR2nKxCkjnA2cCRwFpgRZLlVXX9QLUzgWVV9YkkBwEXA4uA24GXVdW6JAcDlwB7D2x3XFWNTs9QJElTMcwZwGHAmqq6qap+CVwIHD2uTgG79c/nAesAquqqqlrXl68Gdkmy8+Z3W5K0uYYJgL2BWweW1/LbR/EAS4Djk6ylO/o/bYJ2XgV8r6ruHyj7dD/9854kGb7bkqTNNV0XgY8FllbVQmAxcH6SX7ed5EnAB4A3D2xzXFUdAhzeP14/UcNJTkkymmR0/fr109RdSdIwAXAbsM/A8sK+bNDJwDKAqroSmAvMB0iyELgIOKGqvj+2QVXd1v+9G7iAbqrpd1TVOVU1UlUjCxYsGGZMkqQhDBMAK4ADkjwuyU7AMcDycXVuAY4ASHIgXQCsT7I78FXgjKq6Yqxykh2TjAXEw4CXAtdt7mAkScObNACqagNwKt0neG6g+7TP6iRnJXl5X+104E1JrgY+D5xUVdVvtz/w3nEf99wZuCTJNcAqujOKc6d7cJKkjUv3Pj07jIyM1OionxrV1CRhNv13vjHbyzg085KsrKqR8eV+E1iSGmUASFKjJv0msLQ92B6+ZrLHHnts7S5oO2MAaLs3E/Pmzs9rNnIKSJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDVqqABIclSSG5OsSXLGBOv3TXJZkquSXJNkcV9+ZJKVSa7t/75wYJun9eVrknwkSaZvWJKkyUwaAEnmAGcDfwgcBByb5KBx1c4EllXVU4BjgI/35bcDL6uqQ4ATgfMHtvkE8CbggP5x1GaMQ5I0RcOcARwGrKmqm6rql8CFwNHj6hSwW/98HrAOoKquqqp1fflqYJckOyfZC9itqr5TVQV8BnjFZo5FkjQFwwTA3sCtA8tr+7JBS4Djk6wFLgZOm6CdVwHfq6r7++3XTtImAElOSTKaZHT9+vVDdFeSNIzpugh8LLC0qhYCi4Hzk/y67SRPAj4AvHmqDVfVOVU1UlUjCxYsmKbuSpKGCYDbgH0Glhf2ZYNOBpYBVNWVwFxgPkCShcBFwAlV9f2BNhdO0qYkaQsaJgBWAAckeVySnegu8i4fV+cW4AiAJAfSBcD6JLsDXwXOqKorxipX1Q+Bu5L8Qf/pnxOA/73Zo5EkDW3SAKiqDcCpwCXADXSf9lmd5KwkL++rnQ68KcnVwOeBk/qLu6cC+wPvTbKqfzyq3+atwKeANcD3ga9N58AkSQ8t3fv07DAyMlKjo6NbuxvS70jCbPp/SW1JsrKqRsaX+01gSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElq1FABkOSoJDcmWZPkjAnW75vksiRXJbkmyeK+/Pf68nuSfGzcNt/s21zVPx41PUOSJA1jx8kqJJkDnA0cCawFViRZXlXXD1Q7E1hWVZ9IchBwMbAIuA94D3Bw/xjvuKoa3bwhSNMvyYxsU1VT3kaaLpMGAHAYsKaqbgJIciFwNDAYAAXs1j+fB6wDqKqfA5cn2X/aeizNAN+Y1YJhpoD2Bm4dWF7blw1aAhyfZC3d0f9pQ+7/0/30z3uyKYdPkqRNNl0XgY8FllbVQmAxcH6Sydo+rqoOAQ7vH6+fqFKSU5KMJhldv379NHVXkjRMANwG7DOwvLAvG3QysAygqq4E5gLzH6rRqrqt/3s3cAHdVNNE9c6pqpGqGlmwYMEQ3ZUkDWOYAFgBHJDkcUl2Ao4Blo+rcwtwBECSA+kCYKOH60l2TDK/f/4w4KXAdVPvviRpU016EbiqNiQ5FbgEmAOcV1Wrk5wFjFbVcuB04Nwk76C7IHxS9VfRktxMd4F4pySvAF4M/AC4pH/znwN8Azh32kcnSdqozKZPO4yMjNToqJ8alaSpSLKyqkbGl/tNYElqlAEgSY2aVVNASdbTXT+QtjXzgdu3diekjdivqn7nY5SzKgCkbVWS0YnmWKVtmVNAktQoA0CSGmUASNPjnK3dAWmqvAYgSY3yDECSGmUAaEYkuWfg+eIk/5pkvxna92FJvt3fge6qJJ9K8vAkJ42/U91m7ufiJLv3z9+e5IYkn0vy8onupDdN+zwpyfqBO+t9ZnB/SZYkeedA3cduiX5odhrmhjDStElyBPAR4CVVNdR3OpLMqaoHN3F/jwa+CBzT/1ItSV4N7Lop7T2Uqlo8sPhW4EVVtbZfHv8DihuVZMeq2jCFXX+hqk4dVzbR/k6i+9HFdVuwL5pFPAPQjEnyXLof/XtpVX2/Lzs+yT/3R6//s78FKf19pD+Y5GrgmUnem2RFkuuSnDN2A6H+SPv6/l7UF06w27cBfzf25g9QVV+qqh+P69vLkny3P0P4Rh8cJHnewNH1VUl2TbJXf0axqu/P4X3dm5PMT/JJ4PHA15K8Y/BMI8mCJH/fj2VFkmf35UuSnJ/kCuD8zXydf+fMpg+9EeBzfb93SfK0JN9KsjLJJUn26ut+M8nfJhkF/nhz+qJtmwGgmbIz8GXgFVX1L/Drnw5/LfDsqjoUeBA4rq//COC7VfXkqroc+FhVPb2qDgZ2ofsJcYAzgKdU1e8Db5lgvwcDK4fo3+XAH1TVU4ALgXf15e8E3tb373DgF8DrgEv6sicDqwYbqqq30B1lv6CqPjRuPx8GPlRVTwdeBXxqYN1BdGcNxw7R30GvHQipN0xUoaq+BIzS3YjpUGAD8FHg1VX1NOA84P0Dm+zU34fjg1Psi2YRp4A0Ux4A/onu5kFjR5VHAE8DVvQH9LsAP+nXPQj8/cD2L0jyLuDhwJ7AauArwDV0R7VfpguYTbUQ+EJ/FLwT8G99+RXA3yT5HPC/qmptkhXAef3PmX+5qlZN3OSEXgQclN/cAXW3JI/sny+vql9sQt9/awooyUlDbPMEunD8et+XOcAPB9vchH5olvEMQDPlV8BrgMOS/FlfFrrpmUP7xxOqakm/7r6xef8kc4GP0x2tHkI3jTS3r/dHwNnAU+mCZPxBzWq6kJnMR+nOMg4B3jzWflX9NfBGunC6IskTq+rbwHPp7oy3NMkJU3gddqA70xgb895VNXaB/OcTbZDk/WNH+FPYz2QCrB7oxyFV9eKB9RP2RdsXA0AzpqrupXvDPi7JycA/Aq9O8iiAJHtu5JNBY2/2t/dHy6/u6+8A7FNVlwF/CswDHjlu248BJyZ5xlhBkv80Nsc/YB6/udXpiQN1/0NVXVtVH6C7O94T+z7+uKrOpZvCeeoUXoZLgdMG2j90sg2q6s/H3qinsJ+J3M1vLn7fCCxI8sy+Hw9L8qTNbF+zjAGgGVVVdwBHAWcC+/d/L01yDfB1YK8JtvkZ3VH/dXR3plvRr5oDfDbJtcBVwEf6uoPb/pjuNqb/I93HQG8AXkL3ZjhoCfDFJCv57V/1/C/9hd5r6KaxvgY8H7g6yVV01zA+PIWX4O3ASH/R+nomvm6xpSwFPtmfScyhC9IP9BfaVwHPmsG+aBvgN4ElqVGeAUhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIa9f8BXQsrNDzHd0EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Eg8v7PN6t272",
        "outputId": "a92b6990-123a-4713-cce4-d9c0f3fda6b4"
      },
      "source": [
        "# The Keras on Wrapper output\n",
        "#Best: 0.840992 using {'batch_size': 20, 'epochs': 10, 'init': 'glorot_uniform', 'optimizer': 'WAME4'}\n",
        "models2 = []\n",
        "models2.append(('Keras Classifier - Wrapper ', KerasClassifier(build_fn=create_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME4')))\n",
        "# evaluate the model \n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model2 in models2:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True) # Using K Fold cross validation\n",
        "  cv_results = cross_val_score(model2, pca_wrapper_train_features, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm \n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1685 - accuracy: 0.7724\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.8308\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8274\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8347\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1146 - accuracy: 0.8321\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8430\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1099 - accuracy: 0.8425\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.8379\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1097 - accuracy: 0.8405\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1082 - accuracy: 0.8427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1637 - accuracy: 0.7673\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.8232\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8278\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8298\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1137 - accuracy: 0.8309\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.8323\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8340\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8400\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8390\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1110 - accuracy: 0.8402\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1708 - accuracy: 0.7495\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1184 - accuracy: 0.8294\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8303\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.8368\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8311\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.8354\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8389\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8360\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.8420\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.8382\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1756 - accuracy: 0.7657\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.8279\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1154 - accuracy: 0.8298\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8362\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8372\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1136 - accuracy: 0.8351\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8387\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1118 - accuracy: 0.8389\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1111 - accuracy: 0.8398\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1658 - accuracy: 0.7729\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1210 - accuracy: 0.8281\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1149 - accuracy: 0.8323\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1125 - accuracy: 0.8384\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1108 - accuracy: 0.8430\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1103 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1091 - accuracy: 0.8430\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1089 - accuracy: 0.8430\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1651 - accuracy: 0.7692\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.8298\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1161 - accuracy: 0.8312\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8351\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1140 - accuracy: 0.8370\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8384\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8354\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.8409\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.8441\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1105 - accuracy: 0.8437\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1515 - accuracy: 0.7863\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.8316\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8342\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.8286\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8344\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1134 - accuracy: 0.8353\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8422\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.8374\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1121 - accuracy: 0.8404\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1533 - accuracy: 0.7792\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1247 - accuracy: 0.8225\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1177 - accuracy: 0.8295\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.8291\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1160 - accuracy: 0.8339\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1141 - accuracy: 0.8363\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1138 - accuracy: 0.8368\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8361\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8318\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8314\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1664 - accuracy: 0.7706\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1226 - accuracy: 0.8227\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.8282\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8343\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8382\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8346\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1122 - accuracy: 0.8399\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1132 - accuracy: 0.8384\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1123 - accuracy: 0.8392\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1119 - accuracy: 0.8412\n",
            "Epoch 1/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1682 - accuracy: 0.7484\n",
            "Epoch 2/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1191 - accuracy: 0.8282\n",
            "Epoch 3/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1153 - accuracy: 0.8298\n",
            "Epoch 4/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.8307\n",
            "Epoch 5/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1143 - accuracy: 0.8369\n",
            "Epoch 6/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1145 - accuracy: 0.8345\n",
            "Epoch 7/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.8408\n",
            "Epoch 8/10\n",
            "1358/1358 [==============================] - 1s 1ms/step - loss: 0.1127 - accuracy: 0.8362\n",
            "Epoch 9/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1113 - accuracy: 0.8399\n",
            "Epoch 10/10\n",
            "1358/1358 [==============================] - 2s 1ms/step - loss: 0.1130 - accuracy: 0.8381\n",
            "Keras Classifier - Wrapper : 0.838870 (0.005190)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEVCAYAAADkckIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAd40lEQVR4nO3de5xdVYHl8d8iEaI8QjBpRBIIIi3EV7BLGBV8NGqHaAvMh6ETRcGJDbaCgjgSlWkDMzrKCNjKwyZKBxHEoKLBDoKNdKMImBsIj0CjEQJUCFA8FCMihKz+4+zScy6V1K2kKpUU6/v53E+ds/c+++x9k9x1zz73pmSbiIiIXlsM9wAiImLTkmCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoSDDEkJI0T9L/HaK+3yPpynXUv1lS91Cce3Mn6VOSvjbc44hNU4IhBoWkf5f0mKStNtY5bV9o++21MVjSSzfW+VX5iKTbJP1eUrekSyS9cmONYX3Z/pztDwz3OGLTlGCIDSZpMrA/YOBdG+mcozfGefrxT8BHgY8AOwB/CXwfeMdwDqo/m8hzF5uwBEMMhvcB1wPzgCPW1VDSJyStlHS/pA/U3+VLGivpG5J6JN0j6SRJW5S6IyVdK+kMSY8Ac0rZz0r9NeUUN0taJenvauc8QdJD5bzvr5XPk3S2pMvLMddKepGkL5Wrn/+UtPda5rEH8GFgpu2f2P6j7SfKVcznBzif30i6S9LrS/l9ZbxHtI31q5J+LOl3kv5D0q61+n8qxz0uabGk/Wt1cyR9R9I3JT0OHFnKvlnqx5S6R8pYFknasdS9WNICSY9KWibp79v6nV/m+DtJSyV1revPPzYPCYYYDO8DLiyPv+l9UWknaRrwMeCtwEuBN7c1+QowFngJ8KbS7/tr9fsCdwE7Ap+tH2j7jWXz1ba3sf3tsv+i0ufOwCzgLEnjaoceBpwEjAf+CFwH3Fj2vwOcvpY5HwB02/7FWuo7nc8twAuBi4CLgddSPTeHA2dK2qbW/j3A/yljW0L1fPdaBEylunK5CLhE0pha/UFlPtu3HQdVmI8FJpWxfBD4Q6m7GOgGXgwcCnxO0l/Xjn1XabM9sAA4cx3PR2wmEgyxQSTtB+wKzLe9GPg18O61ND8M+BfbS20/Acyp9TMKmAF80vbvbC8HTgPeWzv+fttfsb3a9h/ozNPAKbaftr0QWAW8rFZ/qe3Ftp8ELgWetP0N288A3wb6vGKgegFdubaTdjifu23/S+1ck8pY/2j7SuApqpDo9a+2r7H9R+DTwOskTQKw/U3bj5Tn5jRgq7Z5Xmf7+7bX9PHcPV3m81Lbz5Tn4/HS9xuAE20/aXsJ8DWqgOv1M9sLyxwuAF69tuckNh8JhthQRwBX2n647F/E2peTXgzcV9uvb48HngfcUyu7h+qdfl/tO/WI7dW1/SeA+rvwB2vbf+hjv9620S+w0zrO28l82s+F7XWd/0/zt70KeJTqOUXSxyXdIem3kn5DdQUwvq9j+3ABcAVwcVniO1XS80rfj9r+3Trm8EBt+wlgTO5hbP4SDLHeJD2f6irgTZIekPQAcDzwakl9vXNcCUys7U+qbT9M9c5111rZLsCK2v6m9F8BXwVMXMeaeifzGag/PV9liWkH4P5yP+ETVH8W42xvD/wWUO3YtT535WrqZNtTgNcD76S6Krgf2EHStoM4h9gMJBhiQxwMPANMoVrfngrsBfyU5nJDr/nA+yXtJekFwP/urShLEfOBz0rattxY/RjwzQGM50Gq9fwhZ/tXwNnAt1R9X2LLchN3hqTZgzSfdtMl7SdpS6p7Ddfbvg/YFlgN9ACjJf0jsF2nnUp6i6RXluWvx6kCbU3p++fA/ytzexXVfZoNmUNsBhIMsSGOoLpncK/tB3ofVDcg39O+pGD7cuDLwNXAMqpPMkF10xfgWOD3VDeYf0a1LHXeAMYzBzi/fLLmsPWc00B8hGquZwG/obq/cghwWanf0Pm0uwj4DNUS0l9R3aCGahnoR8AvqZZ6nmRgy24vorox/ThwB/AfVMtLADOByVRXD5cCn7H9bxswh9gMKL+oJ4aLpL2A24Ct2u4DRBtJ86g+BXXScI8lRr5cMcRGJekQSVuVj4x+AbgsoRCxaUkwxMZ2NPAQ1bLLM8A/DO9wIqJdlpIiIqIhVwwREdGQYIiIiIYEQ0RENCQYIiKiIcEQERENCYaIiGhIMEREREOCISIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoGN1/k03f+PHjPXny5OEeRkTEZmXx4sUP257QXj4igmHy5Mm0Wq3hHkZExGZF0j19lWcpKSIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoSDBERETDiPiCW8TGImmjnMf2RjlPRF8SDBEDMNAXbEl5kY/NTpaSIiKiIcEQERENCYaIiGhIMEREREOCISIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDR0Fg6Rpku6UtEzS7D7qd5F0taSbJN0iaXof9askfbzsv0zSktrjcUnHlbo5klbU6qa3ny8iIoZOv/9XkqRRwFnA24BuYJGkBbZvrzU7CZhv+xxJU4CFwORa/enA5b07tu8Eptb6XwFcWmt/hu0vrteMIiJig3RyxbAPsMz2XbafAi4GDmprY2C7sj0WuL+3QtLBwN3A0rX0fwDwa9v3DGTgERExNDoJhp2B+2r73aWsbg5wuKRuqquFYwEkbQOcCJy8jv5nAN9qKzumLEmdJ2lcB2OMiIhBMlg3n2cC82xPBKYDF0jagiowzrC9qq+DJG0JvAu4pFZ8DrA71VLTSuC0tRx7lKSWpFZPT88gTSMiIjr5fQwrgEm1/YmlrG4WMA3A9nWSxgDjgX2BQyWdCmwPrJH0pO0zy3EHAjfafrC3o/q2pLnAD/salO1zgXMBurq68h/eR0QMkk6CYRGwh6TdqAJhBvDutjb3Ut0rmCdpL2AM0GN7/94GkuYAq2qhANWVRmMZSdJOtleW3UOA2zqfTkREbKh+g8H2aknHAFcAo4DzbC+VdArQsr0AOAGYK+l4qhvRR7qfX1slaWuqTzod3VZ1qqSppZ/lfdRHRMQQ0kj4tYNdXV1utVrDPYyIZ8mv9oxNmaTFtrvay/PN54iIaEgwREREQ4IhIiIaEgwREdGQYIiIiIYEQ0RENCQYIiKiIcEQERENCYaIiGhIMEREREOCISIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoSDBERERDgiEiIhoSDBER0dBRMEiaJulOScskze6jfhdJV0u6SdItkqb3Ub9K0sdrZcsl3SppiaRWrXwHST+W9Kvyc9yGTDAiIgam32CQNAo4CzgQmALMlDSlrdlJwHzbewMzgLPb6k8HLu+j+7fYnmq7q1Y2G7jK9h7AVWU/IiI2kk6uGPYBltm+y/ZTwMXAQW1tDGxXtscC9/dWSDoYuBtY2uGYDgLOL9vnAwd3eFxERAyCToJhZ+C+2n53KaubAxwuqRtYCBwLIGkb4ETg5D76NXClpMWSjqqV72h7Zdl+ANixgzFGRMQgGaybzzOBebYnAtOBCyRtQRUYZ9he1ccx+9l+DdUS1YclvbG9gW1TBcizSDpKUktSq6enZ5CmERERnQTDCmBSbX9iKaubBcwHsH0dMAYYD+wLnCppOXAc8ClJx5R2K8rPh4BLqZasAB6UtBNA+flQX4Oyfa7tLttdEyZM6GAaERHRiU6CYRGwh6TdJG1JdXN5QVube4EDACTtRRUMPbb3tz3Z9mTgS8DnbJ8paWtJ25b2WwNvB24rfS0AjijbRwA/WO/ZRUTEgI3ur4Ht1eVd/hXAKOA820slnQK0bC8ATgDmSjqeaunnyLIMtDY7ApdK6h3DRbZ/VOo+D8yXNAu4BzhsPecWERHrQet+/d48dHV1udVq9d8wYiOTxEj4NxYjk6TFbV8XAPLN54iIaJNgiIiIhgRDREQ0JBgiIqIhwRAREQ0JhoiIaEgwREREQ4IhIiIaEgwREdGQYIiIiIYEQ0RENCQYIiKiIcEQERENCYaIiGhIMEREREOCISIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDR0Fg6Rpku6UtEzS7D7qd5F0taSbJN0iaXof9askfbzsTyrtb5e0VNJHa23nSFohaUl5TG8/X0REDJ3R/TWQNAo4C3gb0A0skrTA9u21ZicB822fI2kKsBCYXKs/Hbi8tr8aOMH2jZK2BRZL+nGtzzNsf3G9ZxUREeutkyuGfYBltu+y/RRwMXBQWxsD25XtscD9vRWSDgbuBpb+qbG90vaNZft3wB3Azus7iYiIGDydBMPOwH21/W6e/SI+BzhcUjfV1cKxAJK2AU4ETl5b55ImA3sDN9SKjylLUudJGreW446S1JLU6unp6WAaERHRicG6+TwTmGd7IjAduEDSFlSBcYbtVX0dVILju8Bxth8vxecAuwNTgZXAaX0da/tc2122uyZMmDBI04iIiH7vMQArgEm1/YmlrG4WMA3A9nWSxgDjgX2BQyWdCmwPrJH0pO0zJT2PKhQutP293o5sP9i7LWku8MOBTysiItZXJ8GwCNhD0m5UgTADeHdbm3uBA4B5kvYCxgA9tvfvbSBpDrCqhIKArwN32D693pGknWyvLLuHALcNfFoREbG++g0G26slHQNcAYwCzrO9VNIpQMv2AuAEYK6k46luRB9p2+vo9g3Ae4FbJS0pZZ+yvRA4VdLU0s9y4Oj1nFtERKwHrfv1e/PQ1dXlVqs13MOIeBZJjIR/YzEySVpsu6u9PN98joiIhgRDREQ0JBgiIqIhwRAREQ0JhoiIaEgwREREQydfcIsYkXbYYQcee+yxIT9P9X3OoTNu3DgeffTRIT1HPLckGOI567HHHhsR3zEY6uCJ554sJUVEREOCISIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoSDBERERDgiEiIhoSDBER0ZBgiIiIho6CQdI0SXdKWiZpdh/1u0i6WtJNkm6RNL2P+lWSPt5fn5J2k3RDKf+2pC03ZIIRETEw/QaDpFHAWcCBwBRgpqQpbc1OAubb3huYAZzdVn86cHmHfX4BOMP2S4HHgFkDnVRERKy/Tq4Y9gGW2b7L9lPAxcBBbW0MbFe2xwL391ZIOhi4G1jaX5+q/v/gvwa+U9qdDxw8sClFRMSG6CQYdgbuq+13l7K6OcDhkrqBhcCxAJK2AU4ETu6wzxcCv7G9eh3novR9lKSWpFZPT08H04iIiE4M1s3nmcA82xOB6cAFkragCowzbK8apPP8ie1zbXfZ7powYcJgdx8R8ZzVyW9wWwFMqu1PLGV1s4BpALavkzQGGA/sCxwq6VRge2CNpCeBxWvp8xFge0mjy1VDX+eKiIgh1MkVwyJgj/JpoS2pbi4vaGtzL3AAgKS9gDFAj+39bU+2PRn4EvA522eurU9Xv2fxauDQ0u8RwA82aIYRETEg/QZDeed+DHAFcAfVp4+WSjpF0rtKsxOAv5d0M/At4Eiv45fprq3PUn0i8DFJy6juOXx9/aYWERHrQyPhl6F3dXW51WoN9zBiMyOJkfD3f6TMIzY+SYttd7WX55vPERHRkGCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoSDBERERDgiEiIhoSDBER0ZBgiIiIhgRDREQ0JBgiIqIhwRAREQ0JhoiIaEgwREREQ4IhIiIaEgwREdGQYIiIiIYEQ0RENCQYIiKioaNgkDRN0p2Slkma3Uf9LpKulnSTpFskTS/l+0haUh43SzqklL+sVr5E0uOSjit1cyStqNVNH8wJR0TEuo3ur4GkUcBZwNuAbmCRpAW2b681OwmYb/scSVOAhcBk4Dagy/ZqSTsBN0u6zPadwNRa/yuAS2v9nWH7ixs+vYiIGKhOrhj2AZbZvsv2U8DFwEFtbQxsV7bHAvcD2H7C9upSPqa0a3cA8Gvb9wx08BERMfg6CYadgftq+92lrG4OcLikbqqrhWN7KyTtK2kpcCvwwVpQ9JoBfKut7JiyJHWepHF9DUrSUZJaklo9PT0dTCMiIjoxWDefZwLzbE8EpgMXSNoCwPYNtl8OvBb4pKQxvQdJ2hJ4F3BJra9zgN2plppWAqf1dULb59rust01YcKEQZpGRER0EgwrgEm1/YmlrG4WMB/A9nVUy0bj6w1s3wGsAl5RKz4QuNH2g7V2D9p+xvYaYC7VUlZERGwknQTDImAPSbuVd/gzgAVtbe6luleApL2ogqGnHDO6lO8K7Aksrx03k7ZlpHKTutchVDewIyJiI+n3U0nlE0XHAFcAo4DzbC+VdArQsr0AOAGYK+l4qhvMR9q2pP2A2ZKeBtYAH7L9MICkrak+6XR02ylPlTS19LO8j/qIiBhCsvv6oNDmpaury61Wa7iHEZsZSYyEv/8jZR6x8UlabLurvTzffI6IiIYEQ0RENCQYIiKiIcEQERENCYaIiGhIMEREREOCISIiGhIMERHR0O83nyNGKn9mO5gzdriHscH8me36bxQxAAmGeM7SyY+PiG8MS8JzhnsUMZJkKSkiIhoSDBER0ZBgiIiIhgRDREQ0JBgiIqIhwRAREQ0JhoiIaEgwREREQ4IhIiIa8s3neE6TNNxD2GDjxo0b7iHECNPRFYOkaZLulLRM0uw+6neRdLWkmyTdIml6Kd9H0pLyuFnSIbVjlku6tdS1auU7SPqxpF+Vn/lbH0PC9pA/NsZ5Hn300WF+JmOk6TcYJI0CzgIOBKYAMyVNaWt2EjDf9t7ADODsUn4b0GV7KjAN+GdJ9auUt9iearurVjYbuMr2HsBVZT8iIjaSTq4Y9gGW2b7L9lPAxcBBbW0M9P4Xj2OB+wFsP2F7dSkfU9r15yDg/LJ9PnBwB8dERMQg6SQYdgbuq+13l7K6OcDhkrqBhcCxvRWS9pW0FLgV+GAtKAxcKWmxpKNqfe1oe2XZfgDYsdPJRETEhhusTyXNBObZnghMBy6QtAWA7Rtsvxx4LfBJSWPKMfvZfg3VEtWHJb2xvVNXi7R9XmVIOkpSS1Krp6dnkKYRERGdBMMKYFJtf2Ipq5sFzAewfR3VstH4egPbdwCrgFeU/RXl50PApVRLVgAPStoJoPx8qK9B2T7XdpftrgkTJnQwjYiI6EQnwbAI2EPSbpK2pLq5vKCtzb3AAQCS9qIKhp5yzOhSviuwJ7Bc0taSti3lWwNvp7pRTen7iLJ9BPCD9Z1cREQMXL/fY7C9WtIxwBXAKOA820slnQK0bC8ATgDmSjqeaunnSNuWtB8wW9LTwBrgQ7YflvQS4NLyGfLRwEW2f1RO+XlgvqRZwD3AYYM644iIWCeNhF9t2NXV5Var1X/DiI1M0oj49aExMkla3PZ1ASD/JUZERLRJMEREREOCISIiGhIMERHRkGCIiIiGBENERDQkGCIioiHBEBERDQmGiIhoSDBERERDgiEiIhoSDBER0ZBgiIiIhgRDREQ0JBgiIqIhwRAREQ0JhoiIaEgwREREQ4IhIiIaEgwREdGQYIiIiIaOgkHSNEl3SlomaXYf9btIulrSTZJukTS9lO8jaUl53CzpkFI+qbS/XdJSSR+t9TVH0oracdMHa7IREdG/0f01kDQKOAt4G9ANLJK0wPbttWYnAfNtnyNpCrAQmAzcBnTZXi1pJ+BmSZcBq4ETbN8oaVtgsaQf1/o8w/YXB2uSERHRuU6uGPYBltm+y/ZTwMXAQW1tDGxXtscC9wPYfsL26lI+prTD9krbN5bt3wF3ADtvyEQiImJwdBIMOwP31fa7efaL+BzgcEndVFcLx/ZWSNpX0lLgVuCDtaDorZ8M7A3cUCs+pixJnSdpXF+DknSUpJakVk9PTwfTiIiITgzWzeeZwDzbE4HpwAWStgCwfYPtlwOvBT4paUzvQZK2Ab4LHGf78VJ8DrA7MBVYCZzW1wltn2u7y3bXhAkTBmkaERHRSTCsACbV9ieWsrpZwHwA29dRLRuNrzewfQewCngFgKTnUYXChba/V2v3oO1nbK8B5lItZUVExEbSSTAsAvaQtJukLYEZwIK2NvcCBwBI2osqGHrKMaNL+a7AnsBySQK+Dtxh+/R6R+Umda9DqG5gR0TERtLvp5LKJ4qOAa4ARgHn2V4q6RSgZXsBcAIwV9LxVDeYj7RtSfsBsyU9DawBPmT74VL+XuBWSUvKqT5leyFwqqSppZ/lwNGDOuOIiFgn2R7uMWywrq4ut1qt4R5GxLNIYiT8G4uRSdJi213t5fnmc0RENPS7lBQRf1bdHhv6Y3KVEcMpwRAxAHnBjueCLCVFRERDgiEiIhoSDBER0ZBgiIiIhgRDREQ0JBgiIqIhwRAREQ0JhoiIaBgR/1eSpB7gnuEeR0QfxgMPD/cgItZiV9vP+oU2IyIYIjZVklp9/SdlEZuyLCVFRERDgiEiIhoSDBFD69zhHkDEQOUeQ0RENOSKISIiGhIMMWQkraptT5f0S0m7bqRz7yPpGkl3SrpJ0tckvUDSkZLOHMTzLJS0fdn+iKQ7JF0o6V2SZg/WeWrnk6SHJY0r+ztJ6v396r1teiS9cLDPHc8d+UU9MeQkHQB8Gfgb2x1930TSKNvPrOf5dgQuAWbYvq6UHQpsuz79rYvt6bXdDwFvtd1d9hd02o+k0bZXd3A+S7oeeB2wEHg9cFP5+TNJLwMesf1IrW9RLRuv6XQ8G6rT+cSmKVcMMaQkvRGYC7zT9q9L2eGSfiFpiaR/ljSqlK+SdJqkm4HXSfpHSYsk3Sbp3PIC1/vO/HZJt0i6uI/Tfhg4vzcUAGx/x/aDbWP7W0k3lCuKfyuBgqQ3lbEtKXXblnfm15Sy2yTtX9oulzRe0leBlwCXSzq+fmUiaYKk75a5LJL0hlI+R9IFkq4FLhjA0/pzqiCg/DyDKih696+VNLlcLX0DuA2YJOkcSS1JSyWdXHselks6VdKt5c/lpaV8nqSvlmN+KemdpXyUpP9f5nKLpKNL+Zsl/VTSAuD2AcwnNjW288hjSB7A08CjwKtqZXsBlwHPK/tnA+8r2wYOq7XdobZ9AfC3Zft+YKuyvX0f5/0ecNBaxnQkcGbZHsefP4DxAeC0sn0Z8IayvQ3VlfUJwKdL2Shg27K9HBjfx3b9PBcB+5XtXYA7yvYcYDHw/AE+r28CflK2f1rG2Cr7c4FZwGRgDfDf2p/PMv5/7/1zKePundv7gB+W7XnAj6jeQO4BdANjgKOAk0qbrYAWsBvwZuD3wG7D/Xcvjw17ZCkphtLTVO9uZwEfLWUHAH8FLCoXAM8HHip1zwDfrR3/FkmfAF4A7AAspXrRvgW4UNL3ge9vwPgmAt+WtBOwJXB3Kb8WOF3ShcD3bHdLWgScJ+l5wPdtLxnAed4KTCnzBdhO0jZle4HtPwxw3IuAvSVtTRWwqyTdVd7pvx44rbS7x/b1teMOk3QUVdDtBEyhei4BvlX7eUbtmPmulqB+JekuYE/g7cCryvIcwFiq4HgK+IXtu4nNWpaSYiitAQ4D9pH0qVImqmWeqeXxMttzSt2TLvcVJI2hupo41PYrqd4Jjynt3gGcBbyGKmDa3+AspQqf/nyF6l39K4Gje/u3/XmqK4jnUy3L7Gn7GuCNwApgnqT3DeB52ILqnXvvnHe23Xtj/vd9HSDps73LWe11tp8AfgX8T+DGUnw9MB34C+DO9r4l7QZ8HDjA9quAf+XPzydUV2v9bffuCzi2Np/dbF+5rvnE5iXBEEOqvIi9A3iPpFnAVcChkv4CQNIO6vuTSr0vWg+Xd9eHlvZbAJNsXw2cSPVudZu2Y88EjpC0b2+BpP/eew+hZizVCz3AEbW2u9u+1fYXqN6d71nG+KDtucDXqEKpU1cCx9b6n9rfAbY/3fvCu5YmPweOA3rvo1xHdVV2ve2+vpy0HdWL9m/L83BgW/3f1X5eVyv/H5K2kLQ71T2UO4ErgH8oV09I+sty9RIjRJaSYsjZflTSNOAaqhevk4Ary4v801Q3i+9pO+Y3kuZS3Th9gOoFGqr18W9KGkv1zvXLtn/TduyDkmYAXywBtKac+0dtQ5sDXCLpMeAnVOvkAMdJeks5bilwOTAD+F+SngZWUa3Fd+ojwFmSbqH6N3cN8MEBHN+Xa6mey94X8Ruplsa+1ldj2zdLugn4T+C+cnzduDK+PwIza+X3Ar+gCpYP2n5S0teo7mHcWD4Q0AMcvIHziU1Ivvkc8RwnaTnQZfvhtvJ5VDeivzMc44rhk6WkiIhoyBVDREQ05IohIiIaEgwREdGQYIiIiIYEQ0RENCQYIiKiIcEQEREN/wXvkazA5KOAcAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-2kg0B_suBxg",
        "outputId": "28ed5353-0811-4191-e660-552659979ba8"
      },
      "source": [
        "# The Keras on Embedded Method\n",
        "#Best: 0.833267 using {'batch_size': 40, 'epochs': 10, 'init': 'uniform', 'optimizer': 'WAME3'}\n",
        "models3 = []\n",
        "models3.append(('Keras Classifier - Embedded ', KerasClassifier(build_fn=create_model2, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME3')))\n",
        "# evaluate the model \n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model3 in models3:\n",
        "  kfold = KFold(n_splits=10, random_state=7,shuffle=True) # Using K Fold cross validation\n",
        "  cv_results = cross_val_score(model3, pca_embedded_train_features, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1882 - accuracy: 0.7562\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1308 - accuracy: 0.8096\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1262 - accuracy: 0.8161\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1203 - accuracy: 0.8230\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.8272\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.8262\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8248\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.8264\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1136 - accuracy: 0.8304\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.8304\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1937 - accuracy: 0.7531\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1328 - accuracy: 0.8088\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1293 - accuracy: 0.8162\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1263 - accuracy: 0.8245\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1281 - accuracy: 0.8206\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1251 - accuracy: 0.8252\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1260 - accuracy: 0.8232\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.8263\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1250 - accuracy: 0.8241\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.8282\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1902 - accuracy: 0.7312\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1265 - accuracy: 0.8199\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1245 - accuracy: 0.8236\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1192 - accuracy: 0.8271\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8294\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.8301\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.8374\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8354\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.8369\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.8335\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1887 - accuracy: 0.7296\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1311 - accuracy: 0.8130\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1272 - accuracy: 0.8260\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.8272\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.8208\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.8247\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8256\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.8251\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.8271\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8318\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1908 - accuracy: 0.7416\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1334 - accuracy: 0.8107\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.8245\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1267 - accuracy: 0.8219\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1223 - accuracy: 0.8292\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.8240\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1211 - accuracy: 0.8268\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8302\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1182 - accuracy: 0.8285\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.8281\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7651\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1298 - accuracy: 0.8135\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.8212\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.8269\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.8263\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1143 - accuracy: 0.8310\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8338\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.8351\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.8336\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.8388\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1851 - accuracy: 0.7448\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1323 - accuracy: 0.8162\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1273 - accuracy: 0.8209\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.8193\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.8268\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1203 - accuracy: 0.8238\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.8261\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.8259\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.8295\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.8337\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1858 - accuracy: 0.7507\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1326 - accuracy: 0.8149\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.8222\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.8227\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1240 - accuracy: 0.8264\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1276 - accuracy: 0.8221\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1244 - accuracy: 0.8278\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1250 - accuracy: 0.8240\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1209 - accuracy: 0.8305\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1224 - accuracy: 0.8241\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.7716\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1310 - accuracy: 0.8164\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1283 - accuracy: 0.8218\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1285 - accuracy: 0.8200\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1292 - accuracy: 0.8175\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.8239\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1250 - accuracy: 0.8263\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.8237\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1278 - accuracy: 0.8199\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1262 - accuracy: 0.8251\n",
            "Epoch 1/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.7671\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1326 - accuracy: 0.8170\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1291 - accuracy: 0.8217\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1292 - accuracy: 0.8207\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1290 - accuracy: 0.8196\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1274 - accuracy: 0.8240\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1257 - accuracy: 0.8231\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1258 - accuracy: 0.8238\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1252 - accuracy: 0.8221\n",
            "Epoch 10/10\n",
            "679/679 [==============================] - 1s 1ms/step - loss: 0.1210 - accuracy: 0.8257\n",
            "Keras Classifier - Embedded : 0.830349 (0.004915)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGQ7TJIevlbu"
      },
      "source": [
        "## b) Predicting labels of test data instances and evaluating model based on :\n",
        "\n",
        "(i) Time taken to build model\n",
        "\n",
        "(ii) Time taken to test model\n",
        "\n",
        "(iii) Model Accuracy\n",
        "\n",
        "(iv) Model Error Rate\n",
        "\n",
        "(v) DetectionRate\n",
        "\n",
        "(vi) False Positive\n",
        "\n",
        "(vii) Type II Error\n",
        "\n",
        "(viii) Matthews correlation coefficient (MCC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwzgklanwZy6",
        "outputId": "b7da4922-7bfd-4739-f9e7-cf5db0b4ea7e"
      },
      "source": [
        "# Predicting of Test data of Filter method \n",
        "models1 = []\n",
        "models1.append(('Keras Classifier - Filter Model', KerasClassifier(build_fn=create_model4, batch_size= 20, epochs= 10, init = 'uniform', optimizer='WAME1')))\n",
        "\n",
        "\n",
        "for name, model1 in models1:\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Model : \", name)\n",
        "  model_start = time.time()\n",
        "  model1.fit(pca_filter_train_features, Y)\n",
        "  print(\"Time to build model (sec) : %.4f \" % round(time.time()-model_start,4))\n",
        "  start = time.time()\n",
        "  predicted = model1.predict(pca_filter_test_features)\n",
        "  print(\"Time to test model (sec) : %.4f \" % round(time.time()-start,4))\n",
        "  matrix = confusion_matrix(Y_t, predicted)\n",
        "  print(\"Time elapsed (sec): %.4f \" % round(time.time()-model_start, 4))\n",
        "  print(matrix)\n",
        "\n",
        "  TN1 = matrix[0][0]\n",
        "  FN1 = matrix[1][0]\n",
        "  FP1 = matrix[0][1]\n",
        "  TP1 = matrix[1][1]\n",
        "\n",
        "  DetectionRate_LR = TP1/(TP1+FN1)\n",
        "  Alarm_LR =   FP1/(FP1+TN1)\n",
        "\n",
        "  # To built a MCC for LR\n",
        "  MCC_num_LR= (TP1*TN1)-(FP1*FN1) \n",
        "  MCC_din_LR= math.sqrt((TP1 + FP1)*(TP1+FN1)*(TN1 + FP1)*(TN1+FN1))\n",
        "\n",
        "  MCC_LR = MCC_num_LR / MCC_din_LR\n",
        "  Acc_LR = (TP1 + TN1) / (TP1+FP1+FN1+TN1)\n",
        "  Err_LR = 1 - Acc_LR\n",
        "  T2_Err = (FN1 / (FN1+TP1))\n",
        "  print(\"Model Accuracy : %s\"%(Acc_LR))\n",
        "  print(\"Model Error Rate : %s\"%(Err_LR))\n",
        "  print(\"Detection Rate :%s\"%(DetectionRate_LR))\n",
        "  print(\"False Positive Type 1 :%s\"%(Alarm_LR))\n",
        "  print(\"Type 2 Error : %s\"%(T2_Err))\n",
        "  print(\"Matthews correlation coefficient (MCC):%s\"%(MCC_LR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Model :  Keras Classifier - Filter\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1662 - accuracy: 0.7676\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1261 - accuracy: 0.8129\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1234 - accuracy: 0.8133\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1222 - accuracy: 0.8174\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1196 - accuracy: 0.8253\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1192 - accuracy: 0.8229\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1182 - accuracy: 0.8283\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1175 - accuracy: 0.8256\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1146 - accuracy: 0.8304\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1139 - accuracy: 0.8303\n",
            "Time to build model (sec) : 17.1888 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test model (sec) : 0.4599 \n",
            "Time elapsed (sec): 17.6678 \n",
            "[[10538   822]\n",
            " [ 1738  1962]]\n",
            "Model Accuracy : 0.8300132802124834\n",
            "Model Error Rate : 0.16998671978751656\n",
            "Detection Rate :0.5302702702702703\n",
            "False Positive Type 1 :0.07235915492957747\n",
            "Type 2 Error : 0.4697297297297297\n",
            "Matthews correlation coefficient (MCC):0.5078180787927452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoMw1fYSSmfX",
        "outputId": "acfb2618-702c-4e79-8564-4cc0fa481478"
      },
      "source": [
        "# Predicting of Test data of Wrapper method \n",
        "models2 = []\n",
        "models2.append(('Keras Classifier - Wrapper Model', KerasClassifier(build_fn=create_model5, batch_size= 20, epochs= 10, init = 'glorot_uniform', optimizer='WAME4')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Model : \", name)\n",
        "  model_start = time.time()\n",
        "  model2.fit(pca_wrapper_train_features, Y)\n",
        "  print(\"Time to build model (sec) : %.4f \" % round(time.time()-model_start,4))\n",
        "  start = time.time()\n",
        "  predicted = model2.predict(pca_wrapper_test_features)\n",
        "  print(\"Time to test model (sec) : %.4f \" % round(time.time()-start,4))\n",
        "  matrix = confusion_matrix(Y_t, predicted)\n",
        "  print(\"Time elapsed (sec): %.4f \" % round(time.time()-model_start, 4))\n",
        "  print(matrix)\n",
        "\n",
        "  TN1 = matrix[0][0]\n",
        "  FN1 = matrix[1][0]\n",
        "  FP1 = matrix[0][1]\n",
        "  TP1 = matrix[1][1]\n",
        "\n",
        "  DetectionRate_LR = TP1/(TP1+FN1)\n",
        "  Alarm_LR =   FP1/(FP1+TN1)\n",
        "\n",
        "  # To built a MCC for LR\n",
        "  MCC_num_LR= (TP1*TN1)-(FP1*FN1) \n",
        "  MCC_din_LR= math.sqrt((TP1 + FP1)*(TP1+FN1)*(TN1 + FP1)*(TN1+FN1))\n",
        "\n",
        "  MCC_LR = MCC_num_LR / MCC_din_LR\n",
        "  Acc_LR = (TP1 + TN1) / (TP1+FP1+FN1+TN1)\n",
        "  Err_LR = 1 - Acc_LR\n",
        "  T2_Err = (FN1 / (FN1+TP1))\n",
        "  print(\"Model Accuracy : %s\"%(Acc_LR))\n",
        "  print(\"Model Error Rate : %s\"%(Err_LR))\n",
        "  print(\"Detection Rate :%s\"%(DetectionRate_LR))\n",
        "  print(\"False Positive Type 1 :%s\"%(Alarm_LR))\n",
        "  print(\"Type 2 Error : %s\"%(T2_Err))\n",
        "  print(\"Matthews correlation coefficient (MCC):%s\"%(MCC_LR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Model :  Keras Classifier - Wrapper\n",
            "Epoch 1/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1579 - accuracy: 0.7850\n",
            "Epoch 2/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1195 - accuracy: 0.8291\n",
            "Epoch 3/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1151 - accuracy: 0.8306\n",
            "Epoch 4/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1158 - accuracy: 0.8292\n",
            "Epoch 5/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1144 - accuracy: 0.8275\n",
            "Epoch 6/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1148 - accuracy: 0.8322\n",
            "Epoch 7/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.8366\n",
            "Epoch 8/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1127 - accuracy: 0.8357\n",
            "Epoch 9/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1116 - accuracy: 0.8379\n",
            "Epoch 10/10\n",
            "1509/1509 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.8421\n",
            "Time to build model (sec) : 16.7525 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test model (sec) : 0.4636 \n",
            "Time elapsed (sec): 17.2323 \n",
            "[[10448   912]\n",
            " [ 1539  2161]]\n",
            "Model Accuracy : 0.8372509960159362\n",
            "Model Error Rate : 0.16274900398406378\n",
            "Detection Rate :0.5840540540540541\n",
            "False Positive Type 1 :0.08028169014084507\n",
            "Type 2 Error : 0.41594594594594597\n",
            "Matthews correlation coefficient (MCC):0.5381307807007474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEympxhUTt3t",
        "outputId": "d0c2e7e9-c618-4cd2-b7c7-315f1ddef31b"
      },
      "source": [
        "# Predicting of Test data of Embedded method \n",
        "models3 = []\n",
        "models3.append(('Keras Classifier - Embedded Model', KerasClassifier(build_fn=create_model6, batch_size= 40, epochs= 10, init = 'uniform', optimizer='WAME3')))\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Model : \", name)\n",
        "  model_start = time.time()\n",
        "  model3.fit(pca_embedded_train_features, Y)\n",
        "  print(\"Time to build model (sec) : %.4f \" % round(time.time()-model_start,4))\n",
        "  start = time.time()\n",
        "  predicted = model3.predict(pca_embedded_test_features)\n",
        "  print(\"Time to test model (sec) : %.4f \" % round(time.time()-start,4))\n",
        "  matrix = confusion_matrix(Y_t, predicted)\n",
        "  print(\"Time elapsed (sec): %.4f \" % round(time.time()-model_start, 4))\n",
        "  print(matrix)\n",
        "\n",
        "  TN1 = matrix[0][0]\n",
        "  FN1 = matrix[1][0]\n",
        "  FP1 = matrix[0][1]\n",
        "  TP1 = matrix[1][1]\n",
        "\n",
        "  DetectionRate_LR = TP1/(TP1+FN1)\n",
        "  Alarm_LR =   FP1/(FP1+TN1)\n",
        "\n",
        "  # To built a MCC for LR\n",
        "  MCC_num_LR= (TP1*TN1)-(FP1*FN1) \n",
        "  MCC_din_LR= math.sqrt((TP1 + FP1)*(TP1+FN1)*(TN1 + FP1)*(TN1+FN1))\n",
        "\n",
        "  MCC_LR = MCC_num_LR / MCC_din_LR\n",
        "  Acc_LR = (TP1 + TN1) / (TP1+FP1+FN1+TN1)\n",
        "  Err_LR = 1 - Acc_LR\n",
        "  T2_Err = (FN1 / (FN1+TP1))\n",
        "  print(\"Model Accuracy : %s\"%(Acc_LR))\n",
        "  print(\"Model Error Rate : %s\"%(Err_LR))\n",
        "  print(\"Detection Rate :%s\"%(DetectionRate_LR))\n",
        "  print(\"False Positive Type 1 :%s\"%(Alarm_LR))\n",
        "  print(\"Type 2 Error : %s\"%(T2_Err))\n",
        "  print(\"Matthews correlation coefficient (MCC):%s\"%(MCC_LR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------\n",
            "Model :  Keras Classifier - Embedded\n",
            "Epoch 1/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.7793\n",
            "Epoch 2/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1296 - accuracy: 0.8175\n",
            "Epoch 3/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1249 - accuracy: 0.8222\n",
            "Epoch 4/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.8258\n",
            "Epoch 5/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.8281\n",
            "Epoch 6/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.8288\n",
            "Epoch 7/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.8321\n",
            "Epoch 8/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.8334\n",
            "Epoch 9/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.8336\n",
            "Epoch 10/10\n",
            "755/755 [==============================] - 1s 1ms/step - loss: 0.1120 - accuracy: 0.8341\n",
            "Time to build model (sec) : 8.8648 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to test model (sec) : 0.2903 \n",
            "Time elapsed (sec): 9.1702 \n",
            "[[10369   991]\n",
            " [ 1605  2095]]\n",
            "Model Accuracy : 0.8276228419654714\n",
            "Model Error Rate : 0.17237715803452858\n",
            "Detection Rate :0.5662162162162162\n",
            "False Positive Type 1 :0.08723591549295774\n",
            "Type 2 Error : 0.4337837837837838\n",
            "Matthews correlation coefficient (MCC):0.5108461119040855\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}